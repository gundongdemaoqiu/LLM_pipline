{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/easyocr/detection.py:85: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(copyStateDict(torch.load(trained_model, map_location=device)))\n",
      "/root/miniconda3/lib/python3.10/site-packages/easyocr/recognition.py:182: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n",
      "/root/miniconda3/lib/python3.10/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import json\n",
    "from docling.document_converter import DocumentConverter\n",
    "source = \"/root/code/LLM_pdf/LLM_pipline/10.1016_j.mprp.2018.02.001.pdf\"\n",
    "converter = DocumentConverter()\n",
    "doc = converter.convert_single(source)\n",
    "# print(doc.render_as_dict()) \n",
    "data= doc.render_as_dict()\n",
    "# 提取主文本内容并添加页码信息\n",
    "# 提取主文本内容并添加页码信息\n",
    "def extract_main_text(data):\n",
    "    main_text = []\n",
    "    for element in data.get('main-text', []):\n",
    "        # 检查每个条目是否具有 prov 属性\n",
    "        if 'prov' in element and element['prov']:\n",
    "            page_number = element['prov'][0]['page']\n",
    "        else:\n",
    "            page_number = 'unknown'  # 标记为未知页码\n",
    "        main_text.append({\n",
    "            'page': page_number,\n",
    "            'type': element.get('type', 'unknown'),\n",
    "            'name': element.get('name', 'unknown'),\n",
    "            'text': element.get('text', '')\n",
    "        })\n",
    "    return main_text\n",
    "\n",
    "# def extract_tables(data):\n",
    "#     tables = []\n",
    "#     for page in data.get('content', []):\n",
    "#         page_number = page.get('page_number', 'unknown')\n",
    "#         for table in page.get('elements', []):\n",
    "#             if table['type'] == 'table':\n",
    "#                 table_data = {\n",
    "#                     'page': page_number,\n",
    "#                     'title': table.get('text', ''),\n",
    "#                     'type': table.get('type', 'unknown'),\n",
    "#                     'columns': table.get('#-cols', 0),\n",
    "#                     'rows': table.get('#-rows', 0),\n",
    "#                     'data': []\n",
    "#                 }\n",
    "#                 for row in table.get('data', []):\n",
    "#                     row_data = []\n",
    "#                     for cell in row:\n",
    "#                         row_data.append(cell.get('text', ''))\n",
    "#                     table_data['data'].append(row_data)\n",
    "#                 tables.append(table_data)\n",
    "#     return tables\n",
    "# # 提取表格内容并添加页码信息\n",
    "def extract_tables(data):\n",
    "    tables = []\n",
    "    for table in data.get('tables', []):\n",
    "        page_number = table['prov'][0]['page']\n",
    "        table_data = {\n",
    "            'page': page_number,\n",
    "            'title': table['text'],\n",
    "            'type': table['type'],\n",
    "            'columns': table['#-cols'],\n",
    "            'rows': table['#-rows'],\n",
    "            'data': []\n",
    "        }\n",
    "        for row in table['data']:\n",
    "            row_data = []\n",
    "            for cell in row:\n",
    "                row_data.append(cell['text'])\n",
    "            table_data['data'].append(row_data)\n",
    "        tables.append(table_data)\n",
    "    return tables\n",
    "\n",
    "# 创建最终的JSON结构\n",
    "final_data = {\n",
    "    'filename': data['file-info']['filename'],\n",
    "    'number-of-pages': data['file-info']['#-pages'],\n",
    "    # 'main-text': extract_main_text(data),\n",
    "    'tables': extract_tables(data)\n",
    "}\n",
    "\n",
    "# 格式化输出\n",
    "formatted_json = json.dumps(final_data, indent=4)\n",
    "print(formatted_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## x#lMJìfJJJ = ~I#HFArXivì4>t4# DP\n",
      "\n",
      "\n",
      "\n",
      "论文阅读和聊天功能 :\n",
      "\n",
      "\n",
      "\n",
      "每日 Arxiv 论文推送功能 :\n",
      "\n",
      "用户可以通过提供 pdf 链接或者网页链接等方式，使得聊天机器人对论文内容进行总结，并给予上下文生成回复。\n",
      "\n",
      "定期获取 特定领域的最新发布的论文，现针对 4 种 CS 领域，选取-定数量的匹配度高的论文，利用 LLM\n",
      "\n",
      "## iExàìsfIII ÆIhJhE 1\n",
      "\n",
      " 利用正则化匹配处理输入消息，并针对多样化网址进行处理提取\n",
      "\n",
      " 利用 HTTP 方法以及 PDF 阅读器等方法进行文本内容提取\n",
      "\n",
      " 利用多线程，并且结合上下文信息， LLM 可以针对具体论文给出更准确的理解\n",
      "\n",
      "## 5A Arxivi4 ì##ìXIJJhE 2\n",
      "\n",
      " 定期从 Arxiv 平台上获取最新更新的 CS 领域论文，并利用缓存数据库避免重复加载\n",
      "\n",
      " 结合特定 prompt 提取 4 种特定主题论文，计算标题和摘要嵌入向量并存储于 ChromaDB ，查询时自动匹配 TOP K 匹配度论文\n",
      "\n",
      " 通过修改 prompt 参数，可以支持定制化服务\n",
      "\n",
      "## {4h\n",
      "\n",
      " DP 文献阅读小助手继承了飞书机器人以及 LLM 的功能，提供辅助用户阅读论文的优质体验。\n",
      "\n",
      " 对于定期 Arxiv 论文推送功能，它支持针对不同科学领域的定制服务。\n",
      "\n",
      " 论文阅读功能支持多线程、基于上下文信息的对话功能，为用户提供更真实准确的论文阅读体验 。\n",
      "\n",
      "FIë 446f4F5 #E >#JlMìfHJ] =\n",
      "\n",
      "Developed By: 王健锟、李康宁 DP Technology\n",
      "\n",
      "Jh4\n",
      "\n",
      "## DP'Technology '3Atæ\n"
     ]
    }
   ],
   "source": [
    "print(doc.render_as_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export http_proxy=100.68.173.80:3128\n",
    "!export https_proxy=100.68.173.80:3128"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
