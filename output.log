nohup: ignoring input
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
CUDA extension not installed.
CUDA extension not installed.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
/root/miniconda3/lib/python3.10/site-packages/transformers/modeling_utils.py:4481: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead
  warnings.warn(
/root/code/demo_project
 start API creating

model_name Qwen/Qwen1.5-7B-Chat-GPTQ-Int8
Using device: cuda
Tokenizer for model Qwen/Qwen1.5-7B-Chat-GPTQ-Int8 initialized successfully.


Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  9.57it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 13.40it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 12.87it/s]
Model Qwen/Qwen1.5-7B-Chat-GPTQ-Int8 initialized successfully.



 end API creating
keyword is : test_NLI
question type is : NLI
 the example num is  5116
 the batch size is  52
Start to run model:

Processing NLI Questions:   0%|          | 0/99 [00:00<?, ?it/s]