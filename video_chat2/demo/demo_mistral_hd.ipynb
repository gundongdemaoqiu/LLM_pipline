{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Directory: /root/code/LLM_pipline/video_chat2/demo\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "initial_directory = os.getcwd()\n",
    "print(\"Initial Directory:\", initial_directory)\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.config import Config\n",
    "config_file = \"configs/config_mistral_hd.json\"\n",
    "cfg = Config.from_file(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "from models import VideoChat2_it_hd_mistral\n",
    "from utils.easydict import EasyDict\n",
    "import torch\n",
    "\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from decord import VideoReader, cpu\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import PILToTensor\n",
    "from torchvision import transforms\n",
    "from dataset.video_transforms import (\n",
    "    GroupNormalize, GroupScale, GroupCenterCrop, \n",
    "    Stack, ToTorchFormatTensor\n",
    ")\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import Video, HTML\n",
    "\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import copy\n",
    "\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import decord\n",
    "import time\n",
    "decord.bridge.set_bridge(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d2e85ec58904b3abd5541bbbb1372cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load stage2 model\n",
    "cfg.model.vision_encoder.num_frames = 16\n",
    "model = VideoChat2_it_hd_mistral(config=cfg.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VideoChat2_it_hd_mistral(\n",
       "  (vision_encoder): PretrainVisionTransformer(\n",
       "    (encoder): PretrainVisionTransformerEncoder(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv3d(3, 1024, kernel_size=(1, 16, 16), stride=(1, 16, 16))\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (17): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (18): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (19): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (20): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (21): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (22): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): Identity()\n",
       "    )\n",
       "  )\n",
       "  (vision_layernorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "  (qformer): BertLMHeadModel(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30523, 768)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (drop_path): Identity()\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (drop_path): Identity()\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (drop_path): Identity()\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (intermediate_query): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output_query): BertOutput(\n",
       "              (drop_path): Identity()\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (drop_path): DropPath(p=0.0181818176060915)\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (drop_path): DropPath(p=0.0181818176060915)\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (intermediate_query): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output_query): BertOutput(\n",
       "              (drop_path): DropPath(p=0.0181818176060915)\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (drop_path): DropPath(p=0.036363635212183)\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (drop_path): DropPath(p=0.036363635212183)\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (drop_path): DropPath(p=0.036363635212183)\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (intermediate_query): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output_query): BertOutput(\n",
       "              (drop_path): DropPath(p=0.036363635212183)\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (drop_path): DropPath(p=0.05454545468091965)\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (drop_path): DropPath(p=0.05454545468091965)\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (intermediate_query): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output_query): BertOutput(\n",
       "              (drop_path): DropPath(p=0.05454545468091965)\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (drop_path): DropPath(p=0.072727270424366)\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (drop_path): DropPath(p=0.072727270424366)\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (drop_path): DropPath(p=0.072727270424366)\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (intermediate_query): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output_query): BertOutput(\n",
       "              (drop_path): DropPath(p=0.072727270424366)\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (drop_path): DropPath(p=0.09090908616781235)\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (drop_path): DropPath(p=0.09090908616781235)\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (intermediate_query): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output_query): BertOutput(\n",
       "              (drop_path): DropPath(p=0.09090908616781235)\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (drop_path): DropPath(p=0.10909091681241989)\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (drop_path): DropPath(p=0.10909091681241989)\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (drop_path): DropPath(p=0.10909091681241989)\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (intermediate_query): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output_query): BertOutput(\n",
       "              (drop_path): DropPath(p=0.10909091681241989)\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (drop_path): DropPath(p=0.12727272510528564)\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (drop_path): DropPath(p=0.12727272510528564)\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (intermediate_query): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output_query): BertOutput(\n",
       "              (drop_path): DropPath(p=0.12727272510528564)\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (drop_path): DropPath(p=0.1454545557498932)\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (drop_path): DropPath(p=0.1454545557498932)\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (drop_path): DropPath(p=0.1454545557498932)\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (intermediate_query): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output_query): BertOutput(\n",
       "              (drop_path): DropPath(p=0.1454545557498932)\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (drop_path): DropPath(p=0.16363637149333954)\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (drop_path): DropPath(p=0.16363637149333954)\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (intermediate_query): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output_query): BertOutput(\n",
       "              (drop_path): DropPath(p=0.16363637149333954)\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (drop_path): DropPath(p=0.1818181872367859)\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (drop_path): DropPath(p=0.1818181872367859)\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (drop_path): DropPath(p=0.1818181872367859)\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (intermediate_query): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output_query): BertOutput(\n",
       "              (drop_path): DropPath(p=0.1818181872367859)\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (drop_path): DropPath(p=0.20000000298023224)\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (drop_path): DropPath(p=0.20000000298023224)\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (intermediate_query): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output_query): BertOutput(\n",
       "              (drop_path): DropPath(p=0.20000000298023224)\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (cls): None\n",
       "  )\n",
       "  (mistral_model): MistralForCausalLM(\n",
       "    (model): MistralModel(\n",
       "      (embed_tokens): Embedding(32000, 4096)\n",
       "      (layers): ModuleList(\n",
       "        (0): MistralDecoderLayer(\n",
       "          (self_attn): MistralFlashAttention2(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): MistralRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): MistralMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MistralRMSNorm()\n",
       "          (post_attention_layernorm): MistralRMSNorm()\n",
       "        )\n",
       "        (1): MistralDecoderLayer(\n",
       "          (self_attn): MistralFlashAttention2(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): MistralRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): MistralMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MistralRMSNorm()\n",
       "          (post_attention_layernorm): MistralRMSNorm()\n",
       "        )\n",
       "        (2): MistralDecoderLayer(\n",
       "          (self_attn): MistralFlashAttention2(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): MistralRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): MistralMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MistralRMSNorm()\n",
       "          (post_attention_layernorm): MistralRMSNorm()\n",
       "        )\n",
       "        (3): MistralDecoderLayer(\n",
       "          (self_attn): MistralFlashAttention2(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): MistralRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): MistralMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MistralRMSNorm()\n",
       "          (post_attention_layernorm): MistralRMSNorm()\n",
       "        )\n",
       "        (4): MistralDecoderLayer(\n",
       "          (self_attn): MistralFlashAttention2(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): MistralRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): MistralMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MistralRMSNorm()\n",
       "          (post_attention_layernorm): MistralRMSNorm()\n",
       "        )\n",
       "        (5): MistralDecoderLayer(\n",
       "          (self_attn): MistralFlashAttention2(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): MistralRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): MistralMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MistralRMSNorm()\n",
       "          (post_attention_layernorm): MistralRMSNorm()\n",
       "        )\n",
       "        (6): MistralDecoderLayer(\n",
       "          (self_attn): MistralFlashAttention2(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): MistralRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): MistralMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MistralRMSNorm()\n",
       "          (post_attention_layernorm): MistralRMSNorm()\n",
       "        )\n",
       "        (7): MistralDecoderLayer(\n",
       "          (self_attn): MistralFlashAttention2(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): MistralRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): MistralMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MistralRMSNorm()\n",
       "          (post_attention_layernorm): MistralRMSNorm()\n",
       "        )\n",
       "        (8): MistralDecoderLayer(\n",
       "          (self_attn): MistralFlashAttention2(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): MistralRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): MistralMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MistralRMSNorm()\n",
       "          (post_attention_layernorm): MistralRMSNorm()\n",
       "        )\n",
       "        (9): MistralDecoderLayer(\n",
       "          (self_attn): MistralFlashAttention2(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): MistralRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): MistralMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MistralRMSNorm()\n",
       "          (post_attention_layernorm): MistralRMSNorm()\n",
       "        )\n",
       "        (10): MistralDecoderLayer(\n",
       "          (self_attn): MistralFlashAttention2(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): MistralRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): MistralMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MistralRMSNorm()\n",
       "          (post_attention_layernorm): MistralRMSNorm()\n",
       "        )\n",
       "        (11): MistralDecoderLayer(\n",
       "          (self_attn): MistralFlashAttention2(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): MistralRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): MistralMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MistralRMSNorm()\n",
       "          (post_attention_layernorm): MistralRMSNorm()\n",
       "        )\n",
       "        (12): MistralDecoderLayer(\n",
       "          (self_attn): MistralFlashAttention2(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): MistralRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): MistralMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MistralRMSNorm()\n",
       "          (post_attention_layernorm): MistralRMSNorm()\n",
       "        )\n",
       "        (13): MistralDecoderLayer(\n",
       "          (self_attn): MistralFlashAttention2(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): MistralRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): MistralMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MistralRMSNorm()\n",
       "          (post_attention_layernorm): MistralRMSNorm()\n",
       "        )\n",
       "        (14): MistralDecoderLayer(\n",
       "          (self_attn): MistralFlashAttention2(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): MistralRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): MistralMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MistralRMSNorm()\n",
       "          (post_attention_layernorm): MistralRMSNorm()\n",
       "        )\n",
       "        (15): MistralDecoderLayer(\n",
       "          (self_attn): MistralFlashAttention2(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): MistralRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): MistralMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MistralRMSNorm()\n",
       "          (post_attention_layernorm): MistralRMSNorm()\n",
       "        )\n",
       "        (16): MistralDecoderLayer(\n",
       "          (self_attn): MistralFlashAttention2(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): MistralRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): MistralMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MistralRMSNorm()\n",
       "          (post_attention_layernorm): MistralRMSNorm()\n",
       "        )\n",
       "        (17): MistralDecoderLayer(\n",
       "          (self_attn): MistralFlashAttention2(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): MistralRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): MistralMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MistralRMSNorm()\n",
       "          (post_attention_layernorm): MistralRMSNorm()\n",
       "        )\n",
       "        (18): MistralDecoderLayer(\n",
       "          (self_attn): MistralFlashAttention2(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): MistralRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): MistralMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MistralRMSNorm()\n",
       "          (post_attention_layernorm): MistralRMSNorm()\n",
       "        )\n",
       "        (19): MistralDecoderLayer(\n",
       "          (self_attn): MistralFlashAttention2(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): MistralRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): MistralMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MistralRMSNorm()\n",
       "          (post_attention_layernorm): MistralRMSNorm()\n",
       "        )\n",
       "        (20): MistralDecoderLayer(\n",
       "          (self_attn): MistralFlashAttention2(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): MistralRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): MistralMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MistralRMSNorm()\n",
       "          (post_attention_layernorm): MistralRMSNorm()\n",
       "        )\n",
       "        (21): MistralDecoderLayer(\n",
       "          (self_attn): MistralFlashAttention2(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): MistralRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): MistralMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MistralRMSNorm()\n",
       "          (post_attention_layernorm): MistralRMSNorm()\n",
       "        )\n",
       "        (22): MistralDecoderLayer(\n",
       "          (self_attn): MistralFlashAttention2(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): MistralRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): MistralMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MistralRMSNorm()\n",
       "          (post_attention_layernorm): MistralRMSNorm()\n",
       "        )\n",
       "        (23): MistralDecoderLayer(\n",
       "          (self_attn): MistralFlashAttention2(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): MistralRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): MistralMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MistralRMSNorm()\n",
       "          (post_attention_layernorm): MistralRMSNorm()\n",
       "        )\n",
       "        (24): MistralDecoderLayer(\n",
       "          (self_attn): MistralFlashAttention2(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): MistralRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): MistralMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MistralRMSNorm()\n",
       "          (post_attention_layernorm): MistralRMSNorm()\n",
       "        )\n",
       "        (25): MistralDecoderLayer(\n",
       "          (self_attn): MistralFlashAttention2(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): MistralRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): MistralMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MistralRMSNorm()\n",
       "          (post_attention_layernorm): MistralRMSNorm()\n",
       "        )\n",
       "        (26): MistralDecoderLayer(\n",
       "          (self_attn): MistralFlashAttention2(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): MistralRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): MistralMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MistralRMSNorm()\n",
       "          (post_attention_layernorm): MistralRMSNorm()\n",
       "        )\n",
       "        (27): MistralDecoderLayer(\n",
       "          (self_attn): MistralFlashAttention2(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): MistralRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): MistralMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MistralRMSNorm()\n",
       "          (post_attention_layernorm): MistralRMSNorm()\n",
       "        )\n",
       "        (28): MistralDecoderLayer(\n",
       "          (self_attn): MistralFlashAttention2(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): MistralRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): MistralMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MistralRMSNorm()\n",
       "          (post_attention_layernorm): MistralRMSNorm()\n",
       "        )\n",
       "        (29): MistralDecoderLayer(\n",
       "          (self_attn): MistralFlashAttention2(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): MistralRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): MistralMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MistralRMSNorm()\n",
       "          (post_attention_layernorm): MistralRMSNorm()\n",
       "        )\n",
       "        (30): MistralDecoderLayer(\n",
       "          (self_attn): MistralFlashAttention2(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): MistralRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): MistralMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MistralRMSNorm()\n",
       "          (post_attention_layernorm): MistralRMSNorm()\n",
       "        )\n",
       "        (31): MistralDecoderLayer(\n",
       "          (self_attn): MistralFlashAttention2(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): MistralRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): MistralMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MistralRMSNorm()\n",
       "          (post_attention_layernorm): MistralRMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm): MistralRMSNorm()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "  )\n",
       "  (mistral_proj): Linear(in_features=768, out_features=4096, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 16139.64 MB\n"
     ]
    }
   ],
   "source": [
    "def get_model_size(model):\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "    size_all_mb = (param_size + buffer_size) / (1024 ** 2)\n",
    "    return size_all_mb\n",
    "\n",
    "# Assuming lora_model is your model\n",
    "model_size = get_model_size(model)\n",
    "print(f\": {model_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add lora to run stage3 model\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, inference_mode=False, \n",
    "    r=16, lora_alpha=32, lora_dropout=0.,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "         \"gate_proj\", \"up_proj\", \"down_proj\", \"lm_head\"\n",
    "    ]\n",
    ")\n",
    "model.mistral_model = get_peft_model(model.mistral_model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_IncompatibleKeys(missing_keys=['mistral_model.base_model.model.model.embed_tokens.weight', 'mistral_model.base_model.model.model.layers.0.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.0.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.0.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.0.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.0.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.0.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.0.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.0.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.0.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.1.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.1.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.1.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.1.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.1.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.1.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.1.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.1.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.1.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.2.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.2.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.2.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.2.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.2.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.2.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.2.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.2.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.2.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.3.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.3.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.3.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.3.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.3.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.3.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.3.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.3.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.3.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.4.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.4.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.4.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.4.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.4.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.4.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.4.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.4.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.4.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.5.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.5.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.5.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.5.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.5.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.5.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.5.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.5.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.5.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.6.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.6.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.6.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.6.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.6.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.6.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.6.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.6.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.6.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.7.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.7.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.7.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.7.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.7.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.7.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.7.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.7.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.7.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.8.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.8.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.8.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.8.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.8.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.8.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.8.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.8.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.8.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.9.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.9.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.9.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.9.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.9.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.9.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.9.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.9.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.9.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.10.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.10.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.10.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.10.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.10.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.10.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.10.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.10.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.10.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.11.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.11.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.11.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.11.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.11.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.11.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.11.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.11.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.11.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.12.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.12.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.12.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.12.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.12.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.12.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.12.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.12.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.12.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.13.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.13.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.13.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.13.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.13.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.13.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.13.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.13.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.13.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.14.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.14.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.14.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.14.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.14.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.14.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.14.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.14.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.14.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.15.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.15.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.15.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.15.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.15.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.15.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.15.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.15.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.15.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.16.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.16.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.16.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.16.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.16.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.16.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.16.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.16.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.16.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.17.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.17.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.17.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.17.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.17.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.17.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.17.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.17.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.17.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.18.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.18.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.18.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.18.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.18.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.18.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.18.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.18.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.18.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.19.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.19.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.19.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.19.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.19.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.19.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.19.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.19.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.19.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.20.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.20.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.20.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.20.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.20.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.20.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.20.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.20.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.20.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.21.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.21.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.21.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.21.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.21.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.21.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.21.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.21.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.21.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.22.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.22.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.22.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.22.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.22.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.22.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.22.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.22.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.22.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.23.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.23.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.23.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.23.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.23.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.23.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.23.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.23.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.23.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.24.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.24.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.24.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.24.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.24.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.24.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.24.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.24.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.24.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.25.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.25.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.25.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.25.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.25.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.25.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.25.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.25.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.25.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.26.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.26.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.26.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.26.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.26.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.26.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.26.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.26.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.26.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.27.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.27.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.27.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.27.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.27.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.27.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.27.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.27.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.27.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.28.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.28.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.28.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.28.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.28.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.28.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.28.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.28.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.28.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.29.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.29.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.29.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.29.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.29.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.29.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.29.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.29.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.29.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.30.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.30.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.30.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.30.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.30.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.30.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.30.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.30.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.30.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.31.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.31.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.31.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.31.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.31.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.31.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.31.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.31.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.31.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.norm.weight', 'mistral_model.base_model.model.lm_head.weight'], unexpected_keys=[])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "state_dict = torch.load(\"/vepfs/fs_users/lkn/videochat2/videochat2_hd_mistral_7b_stage4.pth\", map_location=device)\n",
    "\n",
    "if 'model' in state_dict.keys():\n",
    "    msg = model.load_state_dict(state_dict['model'], strict=False)\n",
    "else:\n",
    "    msg = model.load_state_dict(state_dict, strict=False)\n",
    "print(msg)\n",
    "\n",
    "model = model.to(torch.device(cfg.device))\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(conv):\n",
    "    ret = conv.system + conv.sep\n",
    "    for role, message in conv.messages:\n",
    "        if message:\n",
    "            ret += role + \" \" + message + \" \" + conv.sep\n",
    "        else:\n",
    "            ret += role\n",
    "    return ret\n",
    "\n",
    "\n",
    "def get_prompt2(conv):\n",
    "    ret = conv.system + conv.sep\n",
    "    count = 0\n",
    "    for role, message in conv.messages:\n",
    "        count += 1\n",
    "        if count == len(conv.messages):\n",
    "            ret += role + \" \" + message\n",
    "        else:\n",
    "            if message:\n",
    "                ret += role + \" \" + message + \" \" + conv.sep\n",
    "            else:\n",
    "                ret += role\n",
    "    return ret\n",
    "\n",
    "\n",
    "def get_context_emb(conv, model, img_list, answer_prompt=None, print_res=False):\n",
    "    if answer_prompt:\n",
    "        prompt = get_prompt2(conv)\n",
    "    else:\n",
    "        prompt = get_prompt(conv)\n",
    "    if print_res:\n",
    "        print(prompt)\n",
    "    if '<VideoHere>' in prompt:\n",
    "        prompt_segs = prompt.split('<VideoHere>')\n",
    "    else:\n",
    "        prompt_segs = prompt.split('<ImageHere>')\n",
    "    assert len(prompt_segs) == len(img_list) + 1, \"Unmatched numbers of image placeholders and images.\"\n",
    "    with torch.no_grad():\n",
    "        seg_tokens = [\n",
    "            model.mistral_tokenizer(\n",
    "                seg, return_tensors=\"pt\", add_special_tokens=i == 0).to(\"cuda:0\").input_ids\n",
    "            # only add bos to the first seg\n",
    "            for i, seg in enumerate(prompt_segs)\n",
    "        ]\n",
    "        seg_embs = [model.mistral_model.base_model.model.model.embed_tokens(seg_t) for seg_t in seg_tokens]\n",
    "#         seg_embs = [model.mistral_model.model.embed_tokens(seg_t) for seg_t in seg_tokens]\n",
    "    mixed_embs = [emb for pair in zip(seg_embs[:-1], img_list) for emb in pair] + [seg_embs[-1]]\n",
    "    mixed_embs = torch.cat(mixed_embs, dim=1)\n",
    "    return mixed_embs\n",
    "\n",
    "\n",
    "def ask(text, conv):\n",
    "    conv.messages.append([conv.roles[0], text])\n",
    "        \n",
    "\n",
    "class StoppingCriteriaSub(StoppingCriteria):\n",
    "    def __init__(self, stops=[], encounters=1):\n",
    "        super().__init__()\n",
    "        self.stops = stops\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        for stop in self.stops:\n",
    "            if torch.all((stop == input_ids[0][-len(stop):])).item():\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    \n",
    "def answer(conv, model, img_list, do_sample=True, max_new_tokens=200, num_beams=1, min_length=1, top_p=0.9,\n",
    "               repetition_penalty=1.0, length_penalty=1, temperature=1.0, answer_prompt=None, print_res=False):\n",
    "    stop_words_ids = [\n",
    "        torch.tensor([2]).to(\"cuda:0\"),\n",
    "        torch.tensor([29871, 2]).to(\"cuda:0\")]  # '</s>' can be encoded in two different ways.\n",
    "    stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_words_ids)])\n",
    "    \n",
    "    conv.messages.append([conv.roles[1], answer_prompt])\n",
    "    embs = get_context_emb(conv, model, img_list, answer_prompt=answer_prompt, print_res=print_res)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.mistral_model.generate(\n",
    "            inputs_embeds=embs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            stopping_criteria=stopping_criteria,\n",
    "            num_beams=num_beams,\n",
    "            do_sample=do_sample,\n",
    "            min_length=min_length,\n",
    "            top_p=top_p,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            length_penalty=length_penalty,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "    output_token = outputs[0]\n",
    "    if output_token[0] == 0:  # the model might output a unknow token <unk> at the beginning. remove it\n",
    "            output_token = output_token[1:]\n",
    "    if output_token[0] == 1:  # some users find that there is a start token <s> at the beginning. remove it\n",
    "            output_token = output_token[1:]\n",
    "    output_text = model.mistral_tokenizer.decode(output_token, add_special_tokens=False)\n",
    "    output_text = output_text.split('</s>')[0]  # remove the stop sign </s>\n",
    "#     output_text = output_text.split('[/INST]')[-1].strip()\n",
    "    conv.messages[-1][1] = output_text + '</s>'\n",
    "    return output_text, output_token.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.hd_utils import HD_transform_padding, HD_transform_no_padding\n",
    "\n",
    "def get_index(num_frames, num_segments):\n",
    "    seg_size = float(num_frames - 1) / num_segments\n",
    "    start = int(seg_size / 2)\n",
    "    offsets = np.array([\n",
    "        start + int(np.round(seg_size * idx)) for idx in range(num_segments)\n",
    "    ])\n",
    "    return offsets\n",
    "\n",
    "def load_video(video_path, num_segments=8, return_msg=False, resolution=224, hd_num=6, padding=False):\n",
    "    vr = VideoReader(video_path, ctx=cpu(0), num_threads=1)\n",
    "    num_frames = len(vr)\n",
    "    frame_indices = get_index(num_frames, num_segments)\n",
    "\n",
    "    mean = (0.485, 0.456, 0.406)\n",
    "    std = (0.229, 0.224, 0.225)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Lambda(lambda x: x.float().div(255.0)),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "\n",
    "    frames = vr.get_batch(frame_indices)\n",
    "    frames = frames.permute(0, 3, 1, 2)\n",
    "\n",
    "    if padding:\n",
    "        frames = HD_transform_padding(frames.float(), image_size=resolution, hd_num=hd_num)\n",
    "    else:\n",
    "        frames = HD_transform_no_padding(frames.float(), image_size=resolution, hd_num=hd_num)\n",
    "\n",
    "    frames = transform(frames)\n",
    "    print(frames.shape)\n",
    "    \n",
    "    if return_msg:\n",
    "        fps = float(vr.get_avg_fps())\n",
    "        sec = \", \".join([str(round(f / fps, 1)) for f in frame_indices])\n",
    "        # \" \" should be added in the start and end\n",
    "        msg = f\"The video contains {len(frame_indices)} frames sampled at {sec} seconds.\"\n",
    "        return frames, msg\n",
    "    else:\n",
    "        return frames\n",
    "    \n",
    "\n",
    "def load_video_by_segments(video_path, segment_duration=10, num_segments=8, return_msg=False, resolution=224, hd_num=6, padding=False):\n",
    "    \"\"\"\"\"\"\n",
    "    vr = VideoReader(video_path, ctx=cpu(0), num_threads=1)\n",
    "    num_frames = len(vr)\n",
    "    fps = vr.get_avg_fps()\n",
    "    total_duration = num_frames / fps\n",
    "    \n",
    "    # \n",
    "    num_total_segments = int(total_duration // segment_duration)\n",
    "    \n",
    "    mean = (0.485, 0.456, 0.406)\n",
    "    std = (0.229, 0.224, 0.225)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Lambda(lambda x: x.float().div(255.0)),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "    \n",
    "    frames_list = []\n",
    "    msg_list = []\n",
    "    \n",
    "    # \n",
    "    for i in range(num_total_segments):\n",
    "        # print(i)\n",
    "        start_time = i * segment_duration\n",
    "        end_time = start_time + segment_duration\n",
    "        start_frame = int(start_time * fps)\n",
    "        end_frame = int(end_time * fps)\n",
    "        \n",
    "        # \n",
    "        frame_indices = get_index(end_frame - start_frame, num_segments) + start_frame\n",
    "        \n",
    "        if len(frame_indices) > 0:\n",
    "            frames = vr.get_batch(frame_indices)\n",
    "            frames = frames.permute(0, 3, 1, 2)\n",
    "\n",
    "            if padding:\n",
    "                frames = HD_transform_padding(frames.float(), image_size=resolution, hd_num=hd_num)\n",
    "            else:\n",
    "                frames = HD_transform_no_padding(frames.float(), image_size=resolution, hd_num=hd_num)\n",
    "\n",
    "            frames = transform(frames)\n",
    "            frames_list.append(frames)\n",
    "            \n",
    "            if return_msg:\n",
    "                sec = \", \".join([str(round(f / fps, 1)) for f in frame_indices])\n",
    "                msg = f\" {i + 1}  {len(frame_indices)}  {sec} \"\n",
    "                msg_list.append(msg)\n",
    "    \n",
    "    if return_msg:\n",
    "        return frames_list, msg_list\n",
    "    else:\n",
    "        return frames_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sinusoid_encoding_table(n_position=784, d_hid=1024, cur_frame=8, ckpt_num_frame=4, pre_n_position=784): \n",
    "    ''' Sinusoid position encoding table ''' \n",
    "    # TODO: make it with torch instead of numpy \n",
    "    def get_position_angle_vec(position): \n",
    "        return [position / np.power(10000, 2 * (hid_j // 2) / d_hid) for hid_j in range(d_hid)] \n",
    "    \n",
    "    # generate checkpoint position embedding\n",
    "    sinusoid_table = np.array([get_position_angle_vec(pos_i) for pos_i in range(pre_n_position)]) \n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2]) # dim 2i \n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2]) # dim 2i+1 \n",
    "    sinusoid_table = torch.tensor(sinusoid_table, dtype=torch.float, requires_grad=False).unsqueeze(0)\n",
    "    \n",
    "    print(f\"n_position: {n_position}\")\n",
    "    print(f\"pre_n_position: {pre_n_position}\")\n",
    "    \n",
    "    if n_position != pre_n_position:\n",
    "        T = ckpt_num_frame # checkpoint frame\n",
    "        P = 14 # checkpoint size\n",
    "        C = d_hid\n",
    "        new_P = int((n_position // cur_frame) ** 0.5) # testing size\n",
    "        if new_P != 14:\n",
    "            print(f'Pretraining uses 14x14, but current version is {new_P}x{new_P}')\n",
    "            print(f'Interpolate the position embedding')\n",
    "            sinusoid_table = sinusoid_table.reshape(-1, T, P, P, C)\n",
    "            sinusoid_table = sinusoid_table.reshape(-1, P, P, C).permute(0, 3, 1, 2)\n",
    "            sinusoid_table = torch.nn.functional.interpolate(\n",
    "                sinusoid_table, size=(new_P, new_P), mode='bicubic', align_corners=False)\n",
    "            # BT, C, H, W -> BT, H, W, C ->  B, T, H, W, C\n",
    "            sinusoid_table = sinusoid_table.permute(0, 2, 3, 1).reshape(-1, T, new_P, new_P, C)\n",
    "            sinusoid_table = sinusoid_table.flatten(1, 3)  # B, THW, C\n",
    "    \n",
    "    if cur_frame != ckpt_num_frame:\n",
    "        print(f'Pretraining uses 4 frames, but current frame is {cur_frame}')\n",
    "        print(f'Interpolate the position embedding')\n",
    "        T = ckpt_num_frame # checkpoint frame\n",
    "        new_T = cur_frame # testing frame\n",
    "        # interpolate\n",
    "        P = int((n_position // cur_frame) ** 0.5) # testing size\n",
    "        C = d_hid\n",
    "        sinusoid_table = sinusoid_table.reshape(-1, T, P, P, C)\n",
    "        sinusoid_table = sinusoid_table.permute(0, 2, 3, 4, 1).reshape(-1, C, T)  # BHW, C, T\n",
    "        sinusoid_table = torch.nn.functional.interpolate(sinusoid_table, size=new_T, mode='linear')\n",
    "        sinusoid_table = sinusoid_table.reshape(1, P, P, C, new_T).permute(0, 4, 1, 2, 3) # B, T, H, W, C\n",
    "        sinusoid_table = sinusoid_table.flatten(1, 3)  # B, THW, C\n",
    "        \n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 672, 896])\n",
      "n_position: 1568\n",
      "pre_n_position: 784\n",
      "Pretraining uses 4 frames, but current frame is 8\n",
      "Interpolate the position embedding\n",
      "The video contains 8 frames sampled at 0.4, 1.3, 2.3, 3.2, 4.1, 5.0, 5.9, 6.8 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video alt=\"test\" controls><source src=\"/root/code/LLM_pipline/video_chat2/example/test.mp4\" type=\"video/mp4\"></video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vid_path = \"/root/code/LLM_pipline/video_chat2/example/test.mp4\"\n",
    "# # vid_path = \"./demo/example/jesse_dance.mp4\"\n",
    "\n",
    "\n",
    "# num_frame = 8\n",
    "# # num_frame = 16\n",
    "# # resolution = 384\n",
    "# resolution = 224\n",
    "# # hd_num = 6\n",
    "# hd_num = 12\n",
    "# padding = False\n",
    "# vid, msg = load_video(\n",
    "#     vid_path, num_segments=num_frame, return_msg=True, resolution=resolution,\n",
    "#     hd_num=hd_num, padding=padding\n",
    "# )\n",
    "# new_pos_emb = get_sinusoid_encoding_table(n_position=(resolution//16)**2*num_frame, cur_frame=num_frame)\n",
    "# model.vision_encoder.encoder.pos_embed = new_pos_emb\n",
    "\n",
    "# print(msg)\n",
    "    \n",
    "# # The model expects inputs of shape: T x C x H x W\n",
    "# T_, C, H, W = vid.shape\n",
    "# video = vid.reshape(1, T_, C, H, W).to(\"cuda:0\")\n",
    "\n",
    "# img_list = []\n",
    "# with torch.no_grad():\n",
    "#     image_emb, _, _ = model.encode_img(video, \"Watch the video and answer the question.\")\n",
    "# #     image_emb, _, _ = model.encode_img(video, \"\")\n",
    "\n",
    "# img_list.append(image_emb[0])\n",
    "\n",
    "# HTML(f'<video alt=\"test\" controls><source src=\"{vid_path}\" type=\"video/mp4\"></video>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of segments is:  3\n",
      "n_position: 2352\n",
      "pre_n_position: 784\n",
      "Pretraining uses 4 frames, but current frame is 12\n",
      "Interpolate the position embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing video segments:   0%|          | 0/3 [00:00<?, ?it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] <Video><VideoHere></Video> [/INST] Summarize the main story or event of the video. What is happening? [/INST]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] <Video><VideoHere></Video> [/INST] Summarize the main story or event of the video. What is happening? [/INST] The video captures a political rally where a speaker, presumably a politician, is addressing a crowd. The speaker is dressed in a dark suit and a red cap with the word \"MAGA\" printed on it. The crowd is diverse, with individuals wearing various hats, some with the same \"MAGA\" slogan, and others with different slogans. The speaker is gesturing with his hands and appears to be speaking passionately. The background is filled with a sea of people, some holding up signs, and the atmosphere is lively and energetic. </s> [INST] Now describe the main characters in the video. Include details about their clothing or physical appearance. [/INST]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] <Video><VideoHere></Video> [/INST] Summarize the main story or event of the video. What is happening? [/INST] The video captures a political rally where a speaker, presumably a politician, is addressing a crowd. The speaker is dressed in a dark suit and a red cap with the word \"MAGA\" printed on it. The crowd is diverse, with individuals wearing various hats, some with the same \"MAGA\" slogan, and others with different slogans. The speaker is gesturing with his hands and appears to be speaking passionately. The background is filled with a sea of people, some holding up signs, and the atmosphere is lively and energetic. </s> [INST] Now describe the main characters in the video. Include details about their clothing or physical appearance. [/INST] The main character in the video is the speaker, who is wearing a dark suit and a red cap with the word \"MAGA\" printed on it. The crowd consists of individuals wearing various hats, some with the same \"MAGA\" slogan, and others with different slogans. The crowd is diverse in age, gender, and race. </s> [INST] Describe the objects and background elements, including the environment or any significant objects in the video. [/INST]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing video segments:  33%|      | 1/3 [00:10<00:21, 10.60s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] <Video><VideoHere></Video> [/INST] Summarize the main story or event of the video. What is happening? [/INST]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] <Video><VideoHere></Video> [/INST] Summarize the main story or event of the video. What is happening? [/INST] The video captures a political rally with a crowd of people, some holding signs, and a stage with a podium where a speaker is addressing the audience. </s> [INST] Now describe the main characters in the video. Include details about their clothing or physical appearance. [/INST]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] <Video><VideoHere></Video> [/INST] Summarize the main story or event of the video. What is happening? [/INST] The video captures a political rally with a crowd of people, some holding signs, and a stage with a podium where a speaker is addressing the audience. </s> [INST] Now describe the main characters in the video. Include details about their clothing or physical appearance. [/INST] The main characters in the video are the speaker and the audience. The speaker is dressed in a white shirt and a red cap, while the audience is wearing a variety of clothing, including red caps and shirts. </s> [INST] Describe the objects and background elements, including the environment or any significant objects in the video. [/INST]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing video segments:  67%|   | 2/3 [00:15<00:07,  7.89s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.31 GiB (GPU 0; 23.65 GiB total capacity; 18.84 GiB already allocated; 1.53 GiB free; 21.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# print(\"after\",video.device)\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 39\u001b[0m     image_emb, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWatch the video and answer the question.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# embedding\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# img_list.append(image_emb[0])\u001b[39;00m\n\u001b[1;32m     43\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "File \u001b[0;32m~/code/LLM_pipline/video_chat2/models/videochat_mistra/videochat2_it_hd_mistral.py:218\u001b[0m, in \u001b[0;36mVideoChat2_it_hd_mistral.encode_img\u001b[0;34m(self, image, instruction)\u001b[0m\n\u001b[1;32m    215\u001b[0m use_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m T \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    216\u001b[0m input_imgs \u001b[38;5;241m=\u001b[39m input_imgs\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m) \u001b[38;5;66;03m# [B,T,C,H,W] -> [B,C,T,H,W]\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m image_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_imgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m B, T, L, C \u001b[38;5;241m=\u001b[39m image_embeds\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    220\u001b[0m image_embeds \u001b[38;5;241m=\u001b[39m image_embeds\u001b[38;5;241m.\u001b[39mreshape(B, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, C)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/code/LLM_pipline/video_chat2/models/blip2/vit.py:401\u001b[0m, in \u001b[0;36mPretrainVisionTransformer.forward\u001b[0;34m(self, x, use_image)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, use_image\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    400\u001b[0m     T \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m--> 401\u001b[0m     x_vis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_image\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# [B, N_vis, C_e]\u001b[39;00m\n\u001b[1;32m    402\u001b[0m     B, TL, C \u001b[38;5;241m=\u001b[39m x_vis\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    403\u001b[0m     x_vis \u001b[38;5;241m=\u001b[39m x_vis\u001b[38;5;241m.\u001b[39mview(B, T, TL \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m T, C)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/code/LLM_pipline/video_chat2/models/blip2/vit.py:323\u001b[0m, in \u001b[0;36mPretrainVisionTransformerEncoder.forward\u001b[0;34m(self, x, use_image)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, use_image\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 323\u001b[0m     x_vis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x_vis\n",
      "File \u001b[0;32m~/code/LLM_pipline/video_chat2/models/blip2/vit.py:314\u001b[0m, in \u001b[0;36mPretrainVisionTransformerEncoder.forward_features\u001b[0;34m(self, x, use_image)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks):\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_checkpoint \u001b[38;5;129;01mand\u001b[39;00m idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint_num:\n\u001b[0;32m--> 314\u001b[0m         x_vis \u001b[38;5;241m=\u001b[39m \u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_vis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m         x_vis \u001b[38;5;241m=\u001b[39m blk(x_vis)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:249\u001b[0m, in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, *args, **kwargs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected keyword arguments: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(arg \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m kwargs))\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_reentrant:\n\u001b[0;32m--> 249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCheckpointFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _checkpoint_without_reentrant(\n\u001b[1;32m    252\u001b[0m         function,\n\u001b[1;32m    253\u001b[0m         preserve,\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;241m*\u001b[39margs,\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    256\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:107\u001b[0m, in \u001b[0;36mCheckpointFunction.forward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m    104\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(\u001b[38;5;241m*\u001b[39mtensor_inputs)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 107\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mrun_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/code/LLM_pipline/video_chat2/models/blip2/vit.py:126\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma_1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 126\u001b[0m         x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    127\u001b[0m         x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x)))\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/code/LLM_pipline/video_chat2/models/blip2/vit.py:94\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     91\u001b[0m q \u001b[38;5;241m=\u001b[39m q \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\n\u001b[1;32m     92\u001b[0m attn \u001b[38;5;241m=\u001b[39m (q \u001b[38;5;241m@\u001b[39m k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 94\u001b[0m attn \u001b[38;5;241m=\u001b[39m \u001b[43mattn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_drop(attn)\n\u001b[1;32m     97\u001b[0m x \u001b[38;5;241m=\u001b[39m (attn \u001b[38;5;241m@\u001b[39m v)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(B, N, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.31 GiB (GPU 0; 23.65 GiB total capacity; 18.84 GiB already allocated; 1.53 GiB free; 21.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "vid_path = \"/root/code/LLM_pipline/video_chat2/example/trump_long.mp4\"\n",
    "# vid_path = \"./demo/example/jesse_dance.mp4\"\n",
    "\n",
    "num_frame = 10\n",
    "resolution = 224\n",
    "hd_num = 12\n",
    "padding = False\n",
    "segment_duration = 50  # \n",
    "\n",
    "# \n",
    "vid_segments, msg_segments = load_video_by_segments(\n",
    "    vid_path, segment_duration=segment_duration, num_segments=num_frame, return_msg=True,\n",
    "    resolution=resolution, hd_num=hd_num, padding=padding\n",
    ")\n",
    "print(\"length of segments is: \",len(msg_segments))\n",
    "# \n",
    "# for i, msg in enumerate(msg_segments):\n",
    "#     print(f\"Segment {i+1}: {msg}\")\n",
    "\n",
    "# \n",
    "new_pos_emb = get_sinusoid_encoding_table(n_position=(resolution // 16) ** 2 * num_frame, cur_frame=num_frame)\n",
    "\n",
    "# \n",
    "model.vision_encoder.encoder.pos_embed = new_pos_emb\n",
    "\n",
    "# image embeddings\n",
    "# img_list = []\n",
    "out_list = []\n",
    "# \n",
    "\n",
    "for vid in tqdm(vid_segments, desc=\"Processing video segments\"):\n",
    "    \n",
    "    #  T x C x H x W\n",
    "    T_, C, H, W = vid.shape\n",
    "    video = vid.reshape(1, T_, C, H, W).to(\"cuda:0\") \n",
    "    # print(\"after\",video.device)\n",
    "    # \n",
    "    with torch.no_grad():\n",
    "        image_emb, _, _ = model.encode_img(video, \"Watch the video and answer the question.\")\n",
    "    \n",
    "    # embedding\n",
    "    # img_list.append(image_emb[0])\n",
    "    torch.cuda.empty_cache()\n",
    "    chat = EasyDict({\n",
    "    \"system\": \"\",\n",
    "    \"roles\": (\"[INST]\", \"[/INST]\"),\n",
    "    \"messages\": [],\n",
    "    \"sep\": \"\"\n",
    "    })\n",
    "    itr_list = []\n",
    "    chat.messages.append([chat.roles[0], \"<Video><VideoHere></Video> [/INST] Summarize the main story or event of the video. What is happening?\"])\n",
    "    llm_message_1 = answer(conv=chat, model=model, do_sample=False, img_list=[image_emb[0]], max_new_tokens=1024, print_res=True)[0]\n",
    "    itr_list.append(llm_message_1)\n",
    "    \n",
    "    chat.messages.append([chat.roles[0], \"Now describe the main characters in the video. Include details about their clothing or physical appearance.\"])\n",
    "    llm_message_2 = answer(conv=chat, model=model, do_sample=False, img_list=[image_emb[0]], max_new_tokens=1024, print_res=True)[0]\n",
    "    itr_list.append(llm_message_2)\n",
    "    \n",
    "    chat.messages.append([chat.roles[0], \"Describe the objects and background elements, including the environment or any significant objects in the video.\"])\n",
    "    llm_message_3 = answer(conv=chat, model=model, do_sample=False, img_list=[image_emb[0]], max_new_tokens=1024, print_res=True)[0]\n",
    "    itr_list.append(llm_message_3)\n",
    "\n",
    "    out_list.append(itr_list)\n",
    "\n",
    "    ###  model.asnwer......\n",
    "    ##  \n",
    "out_list\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# \n",
    "# HTML(f'<video alt=\"test\" controls><source src=\"{vid_path}\" type=\"video/mp4\"></video>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The video shows a person, presumably a politician, addressing a crowd. The individual is wearing a red cap with the word \"TRUMP\" on it and is holding a microphone, suggesting they are delivering a speech. The crowd is composed of people wearing red caps and holding signs, indicating a political rally or event. ',\n",
       "  'The main character in the video is a person wearing a red cap with the word \"TRUMP\" on it. They are also wearing a dark suit and are holding a microphone, suggesting they are delivering a speech. The crowd in the background is composed of people wearing red caps and holding signs, indicating a political rally or event. ',\n",
       "  'The background of the video is a crowd of people, many of whom are wearing red caps and holding signs. The environment suggests an outdoor setting, possibly a rally or event. The person in the foreground is holding a microphone, indicating they are addressing the crowd. '],\n",
       " ['The video shows a person, presumably a politician, addressing a crowd. The individual is wearing a red cap with the word \"TRUMP\" on it and is speaking into a microphone. The crowd is composed of people wearing red caps and holding signs with messages such as \"MAKE AMERICA GREAT AGAIN\" and \"Joe Biden is a fire.\" The background is blurred, but it appears to be an outdoor event with a clear sky. ',\n",
       "  'The main character in the video is a person wearing a red cap with the word \"TRUMP\" on it. The individual is dressed in a dark suit and is speaking into a microphone. The crowd in the background is composed of people wearing red caps and holding signs with messages such as \"MAKE AMERICA GREAT AGAIN\" and \"Joe Biden is a fire.\" ',\n",
       "  'The background of the video is blurred, but it appears to be an outdoor event with a clear sky. The crowd is composed of people wearing red caps and holding signs with messages such as \"MAKE AMERICA GREAT AGAIN\" and \"Joe Biden is a fire.\" The individual in the foreground is wearing a red cap with the word \"TRUMP\" on it and is speaking into a microphone. '],\n",
       " ['The video shows a person, presumably a politician, addressing a crowd. The individual is wearing a red cap and a suit, and is gesturing with one hand. The crowd is composed of people wearing red caps and holding signs, suggesting a political rally. The background is blurred, but it appears to be an outdoor setting with daylight. ',\n",
       "  'The main character in the video is a person wearing a red cap and a suit. The individual is gesturing with one hand, suggesting they are speaking or addressing the crowd. The crowd in the background is composed of people wearing red caps and holding signs, indicating a political rally. ',\n",
       "  'The background of the video is blurred, but it appears to be an outdoor setting with daylight. The crowd is composed of people wearing red caps and holding signs, suggesting a political rally. The individual in the foreground is wearing a red cap and a suit, and is gesturing with one hand. '],\n",
       " ['The video shows a person, presumably a politician, addressing a crowd. The individual is wearing a suit and a red cap with the word \"TRUMP\" on it. The crowd is composed of people wearing red caps and holding signs with slogans such as \"Make America Great Again\" and \"You\\'re Fired.\" The background is blurred, but it appears to be an outdoor event with a clear sky. ',\n",
       "  'The main character in the video is a person wearing a suit and a red cap with the word \"TRUMP\" on it. The crowd consists of people wearing red caps and holding signs with slogans such as \"Make America Great Again\" and \"You\\'re Fired.\" ',\n",
       "  'The background of the video is blurred, but it appears to be an outdoor event with a clear sky. The crowd is composed of people wearing red caps and holding signs with slogans such as \"Make America Great Again\" and \"You\\'re Fired.\" '],\n",
       " ['The video shows a person, presumably a politician, addressing a crowd. The individual is wearing a red cap with the word \"TRUMP\" on it and a blue suit. The crowd is composed of people wearing red caps and holding signs with messages such as \"Make America Great Again\" and \"Joe Biden Fired.\" The person is gesturing with their hand and appears to be speaking. ',\n",
       "  'The main character in the video is a person wearing a red cap with the word \"TRUMP\" on it and a blue suit. The crowd consists of people wearing red caps and holding signs with messages such as \"Make America Great Again\" and \"Joe Biden Fired.\" ',\n",
       "  'The environment suggests an outdoor event, possibly a rally or protest, with a crowd of people in the background. The signs and clothing of the crowd indicate a political context. '],\n",
       " ['The video shows a person, presumably a politician, addressing a crowd. The individual is wearing a suit and a red cap with the word \"TRUMP\" on it. The crowd is composed of people wearing similar caps and holding signs with slogans such as \"MAKE AMERICA GREAT AGAIN\" and \"YOUR COUNTRY.\" The background is blurred, but it appears to be an outdoor event with a clear sky. ',\n",
       "  'The main character in the video is a person wearing a suit and a red cap with the word \"TRUMP\" on it. The crowd consists of people wearing similar caps and holding signs with slogans such as \"MAKE AMERICA GREAT AGAIN\" and \"YOUR COUNTRY.\" ',\n",
       "  'The background of the video is blurred, but it appears to be an outdoor event with a clear sky. The crowd is composed of people wearing similar caps and holding signs with slogans such as \"MAKE AMERICA GREAT AGAIN\" and \"YOUR COUNTRY.\" '],\n",
       " ['The video shows a person, presumably a politician, addressing a crowd. The individual is wearing a red cap with the word \"Trump\" on it and is speaking into a microphone. The crowd is composed of people wearing similar red caps and holding signs with the same message. The background is blurred, but it appears to be an outdoor event with a clear sky. ',\n",
       "  'The main character in the video is a person wearing a red cap with the word \"Trump\" on it. The individual is dressed in a dark suit and is speaking into a microphone. The crowd in the background is composed of people wearing similar red caps and holding signs with the same message. ',\n",
       "  'The environment suggests an outdoor event, possibly a rally or campaign event. The crowd is composed of people wearing red caps and holding signs with the same message. The background is blurred, but it appears to be an outdoor setting with a clear sky. '],\n",
       " ['The video shows a person speaking at a rally, with a crowd of people in the background holding signs. The person is wearing a red hat and a suit, and is gesturing with their hand. The signs in the crowd have messages such as \"YOU\\'RE FIRED!\" and \"MAKE AMERICA GREAT AGAIN.\" ',\n",
       "  'The main character in the video is a person wearing a red hat and a suit, who is speaking at the rally. The crowd in the background consists of people wearing various hats, including red hats, and holding signs with messages such as \"YOU\\'RE FIRED!\" and \"MAKE AMERICA GREAT AGAIN.\" ',\n",
       "  'The environment suggests an outdoor setting, possibly a rally or public event. The crowd in the background is holding signs with messages such as \"YOU\\'RE FIRED!\" and \"MAKE AMERICA GREAT AGAIN.\" The person speaking is wearing a red hat and a suit, and is gesturing with their hand. '],\n",
       " ['The video shows a person, presumably a politician, addressing a crowd of people. The crowd is holding up signs with the text \"YOUR FIRED!\" and \"TRUMP.\" The person is wearing a red cap with the word \"MAGA\" on it and is speaking into a microphone. ',\n",
       "  'The main character in the video is a person wearing a red cap with the word \"MAGA\" on it. The crowd consists of people wearing red caps and holding signs with the text \"YOUR FIRED!\" and \"TRUMP.\" ',\n",
       "  'The environment suggests an outdoor event with a crowd of people. The signs held by the crowd have the text \"YOUR FIRED!\" and \"TRUMP.\" The person is speaking into a microphone, and the crowd is listening attentively. '],\n",
       " ['The video shows a person, presumably a speaker, addressing an audience. The audience members are holding up signs with the text \"JOE BIDEN YOU\\'RE FIRED!\" and \"TRUMP.\" The speaker is wearing a red cap with the word \"TRUMP\" on it and a blue pin on their lapel. The background is blurred, but it appears to be an outdoor event with a crowd of people. ',\n",
       "  'The main character in the video is the speaker, who is wearing a red cap with the word \"TRUMP\" on it and a blue pin on their lapel. The audience members are holding up signs with the text \"JOE BIDEN YOU\\'RE FIRED!\" and \"TRUMP.\" The audience members are wearing various hats and clothing, including red and blue caps, sunglasses, and casual attire. ',\n",
       "  'The background of the video is blurred, but it appears to be an outdoor event with a crowd of people. The audience members are holding up signs with the text \"JOE BIDEN YOU\\'RE FIRED!\" and \"TRUMP.\" The speaker is standing in front of a podium, and there are other objects and elements in the background that are not clearly visible due to the blur. '],\n",
       " ['The video captures a political rally where a speaker is addressing a crowd. The crowd is holding up signs with messages such as \"JOE BIDEN YOU\\'RE FIRED\" and \"TRUMP 2024.\" The speaker is wearing a red cap with the word \"TRUMP\" on it and is gesturing with his hands while speaking. ',\n",
       "  'The main character in the video is the speaker, who is wearing a red cap with the word \"TRUMP\" on it and a dark suit. The crowd consists of individuals wearing various types of clothing, including red hats, t-shirts, and sunglasses. ',\n",
       "  'The environment suggests an outdoor setting, possibly a stadium or a large open space. The crowd is densely packed, and the signs they are holding are the most prominent objects in the video. The signs have bold, clear text and are held up high for visibility. '],\n",
       " ['The video captures a political rally where a crowd of people is gathered, with some individuals wearing red hats and others holding signs. The focus is on a podium with a banner that reads \"TEXT PENNSYLVANIA TO 88022\" and \"MAKE AMERICA GREAT AGAIN 2024.\" The crowd appears to be engaged in the event, with some individuals standing and others seated. ',\n",
       "  'The main characters in the video are the individuals on the podium. One person is wearing a blue hat and a white shirt, while another is wearing a red hat and a red shirt. The crowd is diverse in terms of clothing, with some wearing red hats and others in casual attire. ',\n",
       "  'The environment suggests an outdoor setting, possibly a stadium or a large open space. The podium is the central object, with the banner being the most prominent element on it. The crowd is the background element, with individuals wearing red hats and holding signs. '],\n",
       " ['The video shows a large crowd of people gathered in a stadium, with some individuals in the foreground wearing red hats and others in the background wearing various colors. The crowd appears to be engaged in a political rally, with a stage in the background featuring a podium and a flag. ',\n",
       "  'The main characters in the video are the individuals in the foreground wearing red hats. They are standing and appear to be actively participating in the event. ',\n",
       "  'The environment suggests an outdoor setting, possibly a stadium or an open field, with a clear sky. The stage in the background features a podium and a flag, indicating a political rally. The crowd is densely packed, with some individuals standing and others seated. '],\n",
       " ['The video shows a large crowd of people gathered in an outdoor setting, with a stage in the foreground. The crowd is predominantly dressed in red and white, with some individuals holding up signs. The stage has a banner with the word \"TRUMP\" prominently displayed. The atmosphere suggests a political rally or event. ',\n",
       "  'The main characters in the video are the individuals on the stage. They are dressed in formal attire, with one person wearing a suit and another in a red shirt. The crowd in the background is predominantly dressed in red and white, with some individuals holding up signs. ',\n",
       "  'The environment suggests an outdoor setting, possibly a stadium or an open field. The stage is decorated with a banner that reads \"TRUMP\" in large letters. The crowd in the background is densely packed, with some individuals holding up signs. The sky is clear, indicating good weather. '],\n",
       " ['The video shows a political rally with a crowd of people, some holding signs, and a speaker at a podium with a banner that reads \"TRUMP FIRED UP.\" ',\n",
       "  'The main character in the video is the speaker at the podium, who is wearing a dark suit. The crowd consists of people wearing various colors, including red, blue, and white. Some individuals are holding signs with messages such as \"TRUMP\" and \"FIRED UP.\" ',\n",
       "  'The environment is an outdoor setting with a crowd of people. The background includes a large banner with the American flag design and stars. The podium is decorated with a banner that reads \"TRUMP FIRED UP.\" '],\n",
       " ['The video shows a political rally with a crowd of people, some holding signs, and a speaker at a podium with a banner that reads \"TRUMP 2020.\" ',\n",
       "  'The main characters in the video are the speaker at the podium and the crowd of people. The speaker is wearing a dark suit and a red cap, while the crowd is dressed in various casual attire, with some wearing hats and holding signs. ',\n",
       "  'The environment is an outdoor setting with a crowd of people, some holding signs, and a speaker at a podium with a banner that reads \"TRUMP 2020.\" The background includes a large American flag with stars and stripes. '],\n",
       " ['The video shows a political rally with a crowd of people, some holding signs, and a speaker on a stage with a large American flag behind him. ',\n",
       "  'The main characters in the video are the speaker on the stage, who is wearing a dark suit and a red cap, and the crowd of people in the background, some of whom are holding signs. ',\n",
       "  'The environment is an outdoor setting with a clear sky, and the background elements include a large American flag with white stars on a blue field and red and white stripes. '],\n",
       " ['The video shows a political rally with a crowd of people, a stage with a podium, and a person speaking. ',\n",
       "  'The main character in the video is a person standing at the podium, who appears to be speaking. The crowd in the background is diverse in age and attire, with some wearing red hats and others in casual clothing. ',\n",
       "  'The environment is an outdoor setting with a crowd of people in the background. The stage has a podium with a microphone, and there is a large American flag with stars and stripes behind the podium. '],\n",
       " ['The video shows a person in a white shirt and red cap standing on a stage with a crowd of people in the background. The person is holding a microphone and appears to be speaking. ',\n",
       "  'The main character in the video is a person wearing a white shirt and a red cap. They are standing on a stage with a crowd of people in the background. ',\n",
       "  'The environment suggests an outdoor event with a crowd of people in the background. The stage has a red and white striped backdrop with stars. The person on the stage is holding a microphone. '],\n",
       " ['The video captures a political rally with a crowd of people gathered to listen to a speaker. The speaker is standing on a stage with a podium, dressed in a white shirt and dark pants, and is addressing the crowd. The crowd is diverse in age and attire, with many wearing red hats and holding up signs. The stage is decorated with American flags and banners, and the atmosphere suggests a public event with a political theme. ',\n",
       "  'The main character in the video is the speaker, who is dressed in a white shirt and dark pants. The crowd is diverse in age and attire, with many wearing red hats and holding up signs. ',\n",
       "  'The environment suggests an outdoor setting, possibly a park or a large open space. The stage is decorated with American flags and banners, and the crowd is seated on bleachers. The crowd is diverse in age and attire, with many wearing red hats and holding up signs. '],\n",
       " [\"The video captures a live event with a large crowd of people gathered in an outdoor setting. The crowd is predominantly dressed in red and white, suggesting a themed event or a team's colors. The audience is seated on a tiered seating arrangement, with some individuals standing. The stage is adorned with a red and white color scheme, matching the crowd's attire. The atmosphere appears to be lively and festive, with people engaged in various activities such as standing, sitting, and conversing. \",\n",
       "  'The main characters in the video are the individuals on the stage. They are dressed in formal attire, with one person wearing a white suit and another in a red shirt. The person in the white suit appears to be addressing the crowd, while the individual in the red shirt is standing nearby. ',\n",
       "  'The environment is an outdoor setting with a clear sky. The stage is decorated with red and white banners and flags, and there are white tents in the background. The crowd is seated on tiered seating, and there are individuals standing in the foreground. '],\n",
       " ['The video shows a large crowd gathered in an outdoor setting, with a stage in the foreground where a person is speaking. The crowd is predominantly dressed in red and white, with some wearing hats. The stage is decorated with red and white bunting and American flags. The person on stage is dressed in a white shirt and is speaking into a microphone. The crowd appears to be listening attentively. ',\n",
       "  'The main character in the video is the person on stage, who is dressed in a white shirt. The crowd is predominantly dressed in red and white, with some wearing hats. ',\n",
       "  'The environment is an outdoor setting with a clear sky. The stage is decorated with red and white bunting and American flags. The crowd is seated on a tiered seating structure. '],\n",
       " ['The video captures a large crowd of people gathered in an outdoor setting, with a stage in the foreground. The crowd is predominantly dressed in red and white, with some individuals wearing hats. The stage is adorned with American flags and a large banner with text. The atmosphere suggests a public event or gathering, possibly a political rally or a sports event. ',\n",
       "  'The main characters in the video are the individuals on the stage. They are dressed in white shirts and appear to be addressing the crowd. Their attire suggests a formal or official capacity, possibly as speakers or organizers of the event. ',\n",
       "  'The environment suggests an outdoor setting, possibly a park or a sports stadium. The stage is decorated with American flags and a large banner with text. The crowd is predominantly dressed in red and white, with some individuals wearing hats. The sky is clear, indicating good weather. '],\n",
       " ['The video captures a large crowd of people gathered in an outdoor setting, with a stage in the foreground. The crowd is predominantly dressed in red and white, with some individuals wearing hats. The stage is adorned with a red and white banner with stars and stripes, and there are speakers and microphones set up. The crowd appears to be engaged in an event, possibly a political rally or a public gathering. ',\n",
       "  'The main characters in the video are the individuals on the stage. They are dressed in formal attire, with one wearing a white shirt and another in a dark suit. The crowd is predominantly dressed in red and white, with some individuals wearing hats. ',\n",
       "  'The environment is an outdoor setting with a clear sky. The stage is set up with speakers and microphones, and there are American flags hanging in the background. The crowd is seated on bleachers, and there are tents and other structures in the background. '],\n",
       " ['The video shows a large crowd gathered in an outdoor setting, with a stage in the foreground where a person is speaking. The crowd is predominantly dressed in red and white, with some individuals wearing hats and sunglasses. The stage is decorated with red and white bunting, and there are American flags visible in the background. The person on the stage appears to be addressing the crowd, and there is a sense of anticipation and excitement in the atmosphere. ',\n",
       "  'The main characters in the video are the person on the stage, who is dressed in a dark suit and tie, and the crowd, which is predominantly dressed in red and white. Some individuals in the crowd are wearing hats and sunglasses. ',\n",
       "  'The environment is an outdoor setting with a clear sky. The stage is decorated with red and white bunting, and there are American flags visible in the background. The crowd is seated on bleachers, and there are some individuals standing in the foreground. '],\n",
       " ['The video shows a political event where a person is being sworn in. ',\n",
       "  'The main characters in the video are the person being sworn in, who is wearing a white shirt with a number on the back, and the person administering the oath, who is dressed in a dark suit. ',\n",
       "  'The environment is an outdoor setting with a crowd of people in the background. The stage is decorated with a red, white, and blue banner with stars. '],\n",
       " ['The video shows a group of people, including a man in a suit and a woman, attempting to move a large, purple podium with the word \"TRUMP\" printed on it. The podium is being held by several individuals, and they are struggling to move it. ',\n",
       "  'The main characters in the video are a man in a suit and a woman. The man is wearing a dark suit and appears to be in his 40s, while the woman is wearing a white blouse and appears to be in her 30s. They are both actively involved in the effort to move the podium. ',\n",
       "  'The environment suggests an outdoor event with a crowd of spectators in the background. The podium is the central object in the video, and it is being held by several individuals. The crowd is visible in the background, and there are no other significant objects or actions taking place. '],\n",
       " ['The video shows a group of people, including a man in a suit, standing on a stage with a podium labeled \"TRUMP 2022.\" The man in the suit is surrounded by others, and they appear to be engaged in a physical altercation. ',\n",
       "  'The main character in the video is a man in a suit who is the center of the physical altercation. He is surrounded by other individuals, some of whom are also in suits. The crowd in the background is diverse, with people wearing various colors and styles of clothing. ',\n",
       "  'The environment suggests an outdoor event, possibly a political rally, given the presence of the podium labeled \"TRUMP 2022.\" The crowd in the background is diverse, with people wearing various colors and styles of clothing. The physical altercation takes place on a stage with a podium labeled \"TRUMP 2022.\" '],\n",
       " ['The video shows a political event where a person is being sworn in. ',\n",
       "  'The main characters in the video are the person being sworn in, who is wearing a dark suit, and the person administering the oath, who is also in a dark suit. ',\n",
       "  'The environment is an outdoor setting with a crowd of spectators. The main objects in the video are the podium, the American flag, and the crowd. '],\n",
       " ['The video captures a moment of a political rally where a person is seen holding a sign with the word \"TRUMP\" on it, surrounded by a crowd of people. The crowd is engaged in the event, with some individuals holding up cameras to record the moment. The person holding the sign is standing on a platform with a backdrop of the American flag. ',\n",
       "  'The main character in the video is the person holding the \"TRUMP\" sign. They are wearing a dark suit and tie. The crowd in the background is diverse in terms of clothing, with some wearing hats and others in casual attire. ',\n",
       "  'The background of the video is a crowd of people, some of whom are holding up cameras to record the event. The platform on which the person holding the \"TRUMP\" sign is standing has a backdrop of the American flag. '],\n",
       " ['The video captures a moment of a political rally where a group of individuals, including a prominent figure, is seen surrounded by a crowd of people. The crowd is holding up signs and cameras, indicating a public event. The prominent figure is wearing a dark suit and is being embraced by other individuals. The crowd is diverse in age and attire, with some wearing red hats and others in casual clothing. The background shows a stage with a podium and a banner with the name \"TRUMP\" on it. ',\n",
       "  'The main characters in the video are a group of individuals, including a prominent figure, who are surrounded by a crowd of people. The prominent figure is wearing a dark suit and is being embraced by other individuals. The crowd is diverse in age and attire, with some wearing red hats and others in casual clothing. ',\n",
       "  'The background elements include a stage with a podium and a banner with the name \"TRUMP\" on it. The crowd is diverse in age and attire, with some wearing red hats and others in casual clothing. The environment suggests a public event, possibly a political rally. '],\n",
       " ['The video shows a political rally with a crowd of people, some of whom are holding up signs. A person in a suit is seen walking on a stage with a podium, and there are other individuals around him. ',\n",
       "  'The main character in the video is a person in a suit who is walking on a stage with a podium. There are other individuals around him, some of whom are holding up signs. ',\n",
       "  'The environment is an outdoor setting with a crowd of people. The stage has a podium with a red, white, and blue banner. There are signs being held up by the crowd, and the person in the suit is walking on the stage. '],\n",
       " ['The video shows a group of people gathered around a stage with a large American flag. The individuals on the stage are dressed in formal attire, and the crowd is holding up signs and taking photos. The atmosphere appears to be celebratory. ',\n",
       "  'The individuals on the stage are dressed in formal attire, with one wearing a dark suit and another in a lighter suit. The crowd is diverse, with people wearing various colors and styles of clothing. ',\n",
       "  'The stage is adorned with a large American flag, and the crowd is holding up signs and taking photos. The environment suggests an outdoor event, possibly a political rally or celebration. '],\n",
       " ['The video shows a sequence of events where a person is assisted by others to climb a stage. ',\n",
       "  'The main characters in the video are individuals who are assisting another person to climb a stage. They are wearing formal attire, with one person wearing a dark suit and another in a lighter-colored suit. ',\n",
       "  'The background elements include a crowd of spectators, some of whom are holding signs and banners. The stage is decorated with a red, white, and blue banner with stars. '],\n",
       " ['The video shows a crowd of people gathered around a stage, with some individuals in the foreground holding up their phones to capture the event. The crowd appears to be engaged in the event, with some individuals wearing hats and others in casual attire. The stage is not clearly visible, but there is a sense of anticipation and excitement in the crowd. ',\n",
       "  'The main characters in the video are the individuals in the foreground who are holding up their phones to capture the event. They are dressed in casual attire, with some wearing hats. The crowd in the background is diverse, with individuals wearing hats, sunglasses, and casual clothing. ',\n",
       "  'The environment suggests an outdoor setting, possibly a public event or gathering. The crowd is densely packed, and the stage is not clearly visible. The individuals in the foreground are holding up their phones, indicating that they are capturing the event. The crowd is diverse, with individuals wearing hats, sunglasses, and casual clothing. '],\n",
       " ['The video shows a crowd of people gathered around a black vehicle, with some individuals in the foreground holding up a red sign with the word \"TRUMP\" on it. The crowd appears to be engaged in a political rally or event. ',\n",
       "  \"The main characters in the video are the individuals in the crowd. They are wearing a variety of clothing, including red hats, t-shirts, and jackets. Some are holding up signs, while others are standing or walking around. The focus is on the crowd's engagement with the event. \",\n",
       "  'The environment suggests an outdoor setting, possibly a street or a designated event area. The crowd is densely packed, and the black vehicle in the background is the focal point of the gathering. The signs and clothing of the individuals in the crowd indicate a political event. '],\n",
       " ['The video shows a crowd of people gathered around a black vehicle with red lights on top. Some individuals are holding up signs with the word \"TRUMP\" on them. The crowd appears to be cheering and waving at the vehicle. ',\n",
       "  'The main characters in the video are the individuals holding up signs with the word \"TRUMP\" on them. They are wearing red hats and shirts. There are also other people in the crowd, some of whom are wearing sunglasses and hats. ',\n",
       "  'The environment in the video is outdoors, with a clear sky. The crowd is gathered around a black vehicle with red lights on top. The individuals in the crowd are holding up signs with the word \"TRUMP\" on them. '],\n",
       " ['The video shows a crowd of people gathered around a black vehicle, with some individuals holding up a red sign with the word \"TRUMP\" in white letters. The scene appears to be a political rally or event. ',\n",
       "  'The main characters in the video are the individuals holding up the \"TRUMP\" sign. They are wearing red hats and shirts, indicating a possible political affiliation. The crowd around them is diverse in age and attire, with some wearing sunglasses and others in casual clothing. ',\n",
       "  'The environment suggests an outdoor setting, possibly a parking lot or a street, with a white building in the background. The crowd is dense, and the atmosphere is lively, with people standing close together and some holding up signs. '],\n",
       " ['The video shows a crowd of people gathered around a vehicle, with some individuals attempting to climb onto the vehicle. The crowd is diverse in age and attire, with some individuals holding signs and flags. The actions of the crowd suggest a protest or demonstration. ',\n",
       "  'The main characters in the video are the individuals attempting to climb onto the vehicle. They are dressed in various attire, including suits and casual clothing. Some are wearing hats, and one individual is holding a sign with the word \"TRUMP\" on it. ',\n",
       "  'The environment suggests an outdoor setting, possibly a street or a public square. The crowd is gathered around a vehicle, which is the focal point of the scene. The individuals in the crowd are holding various objects, including signs and flags. The colors in the video are predominantly the red of the \"TRUMP\" sign, the blue of the crowd\\'s clothing, and the natural colors of the outdoor setting. '],\n",
       " ['The video shows a crowd of people gathered around a vehicle, with some individuals in military-style attire and others in civilian clothing. The scene appears to be dynamic, with people moving and interacting with each other. ',\n",
       "  'The main characters in the video include individuals in military-style attire, which includes helmets and body armor, and others in civilian clothing. The clothing and accessories of the individuals vary, with some wearing sunglasses and others in hats. ',\n",
       "  'The environment suggests an outdoor setting, possibly a public event or gathering. The crowd is densely packed, and there are objects like a vehicle and a flag visible in the background. '],\n",
       " ['The video shows a group of people, including individuals in military attire, interacting with a crowd of onlookers. The crowd is holding up various objects, including a flag and a sign, and appears to be engaged in a protest or demonstration. ',\n",
       "  'The main characters in the video are individuals in military attire, including camouflage uniforms and helmets. They are interacting with a crowd of onlookers who are holding up various objects, including a flag and a sign. ',\n",
       "  'The environment in the video appears to be an outdoor setting with a crowd of onlookers. The crowd is holding up various objects, including a flag and a sign. The individuals in military attire are interacting with the crowd, and there is a red building in the background. '],\n",
       " ['The video shows a crowd of people gathered around a vehicle, with some individuals attempting to reach inside. The scene is dynamic, with people moving and interacting with each other. ',\n",
       "  'The main characters in the video are individuals in the crowd. Some are wearing red hats, while others are in casual attire. The focus is on their actions and interactions with each other and the vehicle. ',\n",
       "  'The environment suggests an outdoor setting, possibly a public event or gathering. The vehicle is the central object, with people surrounding it. The crowd is diverse, with individuals in various clothing, including red hats and casual attire. '],\n",
       " ['The video shows a crowd of people gathered around a vehicle, with some individuals holding up their phones to capture the moment. The crowd appears to be engaged in a collective activity, possibly a public event or gathering. ',\n",
       "  'The main characters in the video are the individuals in the crowd. They are dressed in various casual and formal attire, with some wearing hats and sunglasses. The crowd is diverse in age and gender, and their expressions range from excitement to curiosity. ',\n",
       "  'The environment suggests an outdoor setting, possibly a public event or gathering. The crowd is densely packed, and the vehicle they are gathered around is dark-colored. The background is not clearly visible due to the focus on the crowd and the vehicle. ']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] <Video><VideoHere></Video> [/INST] [INST] Describe the video in the format:1.Introduction, 2.character, 3.objects [/INST]\n",
      "1. Introduction: The video opens with a blurry shot of a street at night, with rain pouring down. 2. Character: A man dressed in a dark suit and tie is seen holding an umbrella. 3. Objects: The man is holding a black umbrella and is standing in front of a storefront with the words \"First Edition\" visible. \n"
     ]
    }
   ],
   "source": [
    "chat = EasyDict({\n",
    "    \"system\": \"\",\n",
    "    \"roles\": (\"[INST]\", \"[/INST]\"),\n",
    "    \"messages\": [],\n",
    "    \"sep\": \"\"\n",
    "})\n",
    "\n",
    "chat.messages.append([chat.roles[0], \"<Video><VideoHere></Video> [/INST]\"])\n",
    "# chat.messages.append([chat.roles[0], f\"<Video><VideoHere></Video> {msg} [/INST]\"])\n",
    "ask(\"Describe the video in the format:1.Introduction, 2.character, 3.objects\", chat)\n",
    "#\n",
    "llm_message = answer(conv=chat, model=model, do_sample=False, img_list=img_list, max_new_tokens=512, print_res=True)[0]\n",
    "print(llm_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_path = \"./demo/example/run.jpg\"\n",
    "img_path = \"./demo/example/dog.png\"\n",
    "img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "plt.imshow(img)\n",
    "\n",
    "resolution = 224\n",
    "# resolution = 384\n",
    "# hd_num = 6\n",
    "hd_num = 12\n",
    "padding = False\n",
    "\n",
    "new_pos_emb = get_sinusoid_encoding_table(n_position=(resolution//16)**2, cur_frame=1, ckpt_num_frame=1, pre_n_position=14*14)\n",
    "model.vision_encoder.encoder.img_pos_embed = new_pos_emb\n",
    "\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: x.float().div(255.0)),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "img = PILToTensor()(img).unsqueeze(0)\n",
    "\n",
    "if padding:\n",
    "    img = HD_transform_padding(img.float(), image_size=resolution, hd_num=hd_num)\n",
    "else:\n",
    "    img = HD_transform_no_padding(img.float(), image_size=resolution, hd_num=hd_num)\n",
    "    \n",
    "img = transform(img).unsqueeze(0).cuda()\n",
    "print(img.shape)\n",
    "\n",
    "img_list = []\n",
    "with torch.no_grad():\n",
    "#     image_emb, _, _ = model.encode_img(img, \"\")\n",
    "    image_emb, _, _ = model.encode_img(img, \"Observe the image and answer the question.\")\n",
    "img_list.append(image_emb[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = EasyDict({\n",
    "    \"system\": \"\",\n",
    "    \"roles\": (\"[INST]\", \"[/INST]\"),\n",
    "    \"messages\": [],\n",
    "    \"sep\": \"\"\n",
    "})\n",
    "\n",
    "chat.messages.append([chat.roles[0], f\"<Image><ImageHere></Image> [/INST]\"])\n",
    "ask(\"Describe the following image in details.\", chat)\n",
    "\n",
    "llm_message = answer(conv=chat, model=model, do_sample=False, img_list=img_list, max_new_tokens=256, print_res=True,)[0]\n",
    "print(llm_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_answer_egoschema(pred, qid):\n",
    "    correct = 0\n",
    "    answer_content = ans_dict[qid]['content'].lower()\n",
    "    if answer_content[-1] == \".\":\n",
    "        answer_content = answer_content[:-1]\n",
    "    if ans_dict[qid]['answer'].lower() in pred.lower():\n",
    "        flag = True\n",
    "        for kk in [\"(A)\", \"(B)\", \"(C)\", \"(D)\", \"(E)\"]:\n",
    "            if kk != ans_dict[qid]['answer'].lower() and kk in pred.lower():\n",
    "                flag = ans_dict\n",
    "                break\n",
    "        if flag:\n",
    "            correct += 1\n",
    "    elif answer_content in pred.lower():\n",
    "        correct = 1\n",
    "    elif answer_content.replace(\"a \", \"\") in pred.lower():\n",
    "        correct = 1\n",
    "    elif answer_content.replace(\"an \", \"\") in pred.lower():\n",
    "        correct = 1\n",
    "    return correct\n",
    "\n",
    "def infer_egoschema(\n",
    "        data_sample, system=\"\", \n",
    "        question_prompt='', # add in the end of question\n",
    "        answer_prompt=None, # add in the begining of answer\n",
    "        return_prompt='',  # add in the begining of return message\n",
    "        system_q=False, # whether add question in the system prompt for QFormer\n",
    "        print_res=True,\n",
    "        system_llm=False,\n",
    "        num_segments=8,\n",
    "    ):\n",
    "    vid_path = os.path.join(\"your_data_path/egoschema/videos\", data_sample['video'])\n",
    "    video, _ = load_video(vid_path, num_segments=num_segments, return_msg=True)\n",
    "    T_, C, H, W = video.shape\n",
    "    video = video.reshape(1, T_, C, H, W).to(\"cuda:0\")\n",
    "    \n",
    "    video_list = []\n",
    "    with torch.no_grad():\n",
    "        if system_q:\n",
    "            video_emb, _, _ = model.encode_img(video, system + data_sample['question'])\n",
    "        else:\n",
    "            video_emb, _, _ = model.encode_img(video, system)\n",
    "    video_list.append(video_emb[0])\n",
    "\n",
    "    chat = EasyDict({\n",
    "        \"system\": system,\n",
    "        \"roles\": (\"[INST]\", \"[/INST]\"),\n",
    "        \"messages\": [],\n",
    "        \"sep\": \"\"\n",
    "    })\n",
    "\n",
    "    chat.messages.append([chat.roles[0], f\"<Video><VideoHere></Video> [/INST]\"])\n",
    "    \n",
    "    if system_llm:\n",
    "        prompt = system + data_sample['QA'][0]['q'] + question_prompt\n",
    "    else:\n",
    "        prompt = data_sample['QA'][0]['q'] + question_prompt\n",
    "    \n",
    "    ask(prompt, chat)\n",
    "\n",
    "    llm_message = answer(\n",
    "        conv=chat, model=model, do_sample=False, \n",
    "        img_list=video_list, max_new_tokens=100, \n",
    "        answer_prompt=answer_prompt, print_res=print_res\n",
    "    )[0]\n",
    "    # remove potential explanation\n",
    "    llm_message = return_prompt + llm_message.strip().split('\\n')[0]\n",
    "    print(llm_message)\n",
    "    print(f\"GT: {data_sample['QA'][0]['a']}\")\n",
    "    return llm_message\n",
    "\n",
    "\n",
    "import csv\n",
    "# You can find the csv files in https://github.com/imagegridworth/IG-VLM/blob/main/data/multiple_choice_qa/EgoSchema.csv\n",
    "with open(\"your_data_path/EgoSchema.csv\", mode='r', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "\n",
    "    json_data = []\n",
    "    ans_dict = {}\n",
    "    \n",
    "    for idx, msg in enumerate(reader):\n",
    "        if idx == 0:\n",
    "            print(msg)\n",
    "            continue\n",
    "            \n",
    "        video = msg[1] + '.mp4'\n",
    "        input_str = f\"Question: {msg[3].capitalize()}\\nOptions:\\n\"\n",
    "    \n",
    "        target_index = -1\n",
    "        for i, candidate in enumerate(msg[5:]):\n",
    "            option = chr(ord('A') + i)\n",
    "            input_str += f\"({option}) {candidate}\\n\"\n",
    "            if candidate == msg[4]:\n",
    "                target_index = i\n",
    "            \n",
    "        assert target_index != -1\n",
    "        correct = chr(ord('A') + target_index)\n",
    "        \n",
    "        json_data.append({\n",
    "            'video': video,\n",
    "            \"QA\": [{\n",
    "                \"i\": \"\",\n",
    "                \"q\": input_str.strip(),\n",
    "                \"a\": f\"Answer: ({correct}) {msg[4]}\",\n",
    "            }]\n",
    "        })\n",
    "\n",
    "        ans_dict[idx - 1] = {\n",
    "            'video': video,\n",
    "            'answer': f\"({correct})\",\n",
    "            'content': msg[4],\n",
    "        }\n",
    "\n",
    "\n",
    "#  position embedding\n",
    "num_frame = 16\n",
    "resolution = 224\n",
    "new_pos_emb = get_sinusoid_encoding_table(n_position=(resolution//16)**2*num_frame, cur_frame=num_frame)\n",
    "model.vision_encoder.encoder.pos_embed = new_pos_emb\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "total_num = len(json_data)\n",
    "\n",
    "output = \"\"\n",
    "\n",
    "for idx, example in enumerate(tqdm(json_data)):\n",
    "    start = time.time()\n",
    "    llm_message = infer_egoschema(\n",
    "        example, \n",
    "        \"Carefully watch the video and pay attention to the cause and sequence of events, the detail and movement of objects, and the action and pose of persons. Based on your observations, select the best option that accurately addresses the question.\\n\", \n",
    "        question_prompt=\"\\nOnly give the best option.\", \n",
    "        answer_prompt=\"Best option:(\",\n",
    "        return_prompt='(',\n",
    "        system_q=False,\n",
    "        print_res=False,\n",
    "        system_llm=False,\n",
    "        num_segments=16\n",
    "    )\n",
    "    \n",
    "    duration = time.time() - start\n",
    "    output += (example[\"video\"] + '\\n')\n",
    "    output += (llm_message + '\\n')\n",
    "    correct += check_answer_egoschema(llm_message, idx)\n",
    "    total += 1\n",
    "    print(\"Acc:\", correct / total)\n",
    "    print('-' * 20, f'{idx+1}/{total_num} done,', f'cost: {duration:.2f}s', '-' * 20)\n",
    "\n",
    "with open(\"./demo/egoschema/your_prediction.txt\", \"w\") as f:\n",
    "    f.writelines(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can find the csv files in https://github.com/egoschema/EgoSchema/blob/main/questions.json\n",
    "with open(\"your_data_path/EgoSchema/questions.json\", \"r\") as f:\n",
    "    full_data = json.load(f)\n",
    "\n",
    "full_egoschema = []\n",
    "for data in full_data:\n",
    "    video = data['q_uid'] + '.mp4'\n",
    "    input_str = f\"Question: {data['question'].capitalize()}\\nOptions:\\n\"\n",
    "\n",
    "    for i, candidate in enumerate(['option 0', 'option 1', 'option 2', 'option 3', 'option 4']):\n",
    "        option = chr(ord('A') + i)\n",
    "        input_str += f\"({option}) {data[candidate]}\\n\"\n",
    "    \n",
    "    full_egoschema.append({\n",
    "        'q_uid': data['q_uid'],\n",
    "        'video': video,\n",
    "        \"QA\": [{\n",
    "            \"i\": \"\",\n",
    "            \"q\": input_str.strip(),\n",
    "            \"a\": \"\",\n",
    "        }]\n",
    "    })\n",
    "\n",
    "\n",
    "def infer_full_egoschema(\n",
    "        data_sample, system=\"\", \n",
    "        question_prompt='', # add in the end of question\n",
    "        answer_prompt=None, # add in the begining of answer\n",
    "        return_prompt='',  # add in the begining of return message\n",
    "        system_q=False, # whether add question in the system prompt for QFormer\n",
    "        print_res=True,\n",
    "        system_llm=False,\n",
    "        num_segments=8,\n",
    "    ):\n",
    "    vid_path = os.path.join(\"your_data_path/egoschema/videos\", data_sample['video'])\n",
    "    print(vid_path)\n",
    "    video, _ = load_video(vid_path, num_segments=num_segments, return_msg=True)\n",
    "    T_, C, H, W = video.shape\n",
    "    video = video.reshape(1, T_, C, H, W).to(\"cuda:0\")\n",
    "    \n",
    "    video_list = []\n",
    "    with torch.no_grad():\n",
    "        if system_q:\n",
    "            video_emb, _, _ = model.encode_img(video, system + data_sample['question'])\n",
    "        else:\n",
    "            video_emb, _, _ = model.encode_img(video, system)\n",
    "    video_list.append(video_emb[0])\n",
    "\n",
    "    chat = EasyDict({\n",
    "        \"system\": system,\n",
    "        \"roles\": (\"[INST]\", \"[/INST]\"),\n",
    "        \"messages\": [],\n",
    "        \"sep\": \"\"\n",
    "    })\n",
    "\n",
    "    chat.messages.append([chat.roles[0], f\"<Video><VideoHere></Video> [/INST]\"])\n",
    "    \n",
    "    if system_llm:\n",
    "        prompt = system + data_sample['QA'][0]['q'] + question_prompt\n",
    "    else:\n",
    "        prompt = data_sample['QA'][0]['q'] + question_prompt\n",
    "    \n",
    "    ask(prompt, chat)\n",
    "\n",
    "    llm_message = answer(\n",
    "        conv=chat, model=model, do_sample=False, \n",
    "        img_list=video_list, max_new_tokens=100, \n",
    "        answer_prompt=answer_prompt, print_res=print_res\n",
    "    )[0]\n",
    "    # remove potential explanation\n",
    "    llm_message = return_prompt + llm_message.strip().split('\\n')[0]\n",
    "    print(llm_message)\n",
    "    return llm_message\n",
    "\n",
    "\n",
    "#  position embedding\n",
    "num_frame = 16\n",
    "resolution = 224\n",
    "new_pos_emb = get_sinusoid_encoding_table(n_position=(resolution//16)**2*num_frame, cur_frame=num_frame)\n",
    "model.vision_encoder.encoder.pos_embed = new_pos_emb\n",
    "\n",
    "\n",
    "ans_dict = {}\n",
    "\n",
    "for idx, example in enumerate(tqdm(full_egoschema)):\n",
    "    start = time.time()\n",
    "    llm_message = infer_full_egoschema(\n",
    "        example, \n",
    "        \"Carefully watch the video and pay attention to the cause and sequence of events, the detail and movement of objects, and the action and pose of persons. Based on your observations, select the best option that accurately addresses the question.\\n\", \n",
    "        question_prompt=\"\\nOnly give the best option.\", \n",
    "        answer_prompt=\"Best option:(\",\n",
    "        return_prompt='(',\n",
    "        system_q=False,\n",
    "        print_res=False,\n",
    "        system_llm=False,\n",
    "        num_segments=16,\n",
    "    )\n",
    "\n",
    "    assert llm_message[0] == '(' and llm_message[2] == ')'\n",
    "    ans = ord(llm_message[1]) - ord('A')\n",
    "    assert ans in [0, 1, 2, 3, 4]\n",
    "    ans_dict[example['q_uid']] = ans\n",
    "\n",
    "\n",
    "with open(\"./demo/egoschema/your_prediction.json\", \"w\") as f:\n",
    "    json.dump(ans_dict, f)\n",
    "\n",
    "# Then you can run https://github.com/egoschema/EgoSchema/blob/main/validate.py to get the score\n",
    "# python3 validate.py --f ./your_prediction.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webvtt\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r'[^A-Za-z0-9\\s]', '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def read_vtt_and_concatenate(file_path, tokenizer, max_len=4096):\n",
    "    prev = \"\"\n",
    "    subtitles = []\n",
    "    for caption in webvtt.read(file_path):\n",
    "        # Split the caption text into individual lines\n",
    "        lines = caption.text.split('\\n')\n",
    "        for line in lines:\n",
    "            # Clean the text and check for repetition\n",
    "            line = clean_text(line)\n",
    "            if prev != line and line:\n",
    "                subtitles.append(line)\n",
    "                prev = line\n",
    "\n",
    "    # Join subtitles to check length\n",
    "    full_text = ' '.join(subtitles)\n",
    "    tokenized_ids = tokenizer(full_text, add_special_tokens=False).input_ids\n",
    "\n",
    "    # If the tokenized length is within the limit, return the full text\n",
    "    if len(tokenized_ids) <= max_len:\n",
    "        return full_text\n",
    "\n",
    "    # Otherwise, we need to trim the text to fit within the limit\n",
    "    # We will keep the first half and the last half\n",
    "    half_len = max_len // 2\n",
    "    start_text = ' '.join(subtitles[:half_len])\n",
    "    end_text = ' '.join(subtitles[-half_len:])\n",
    "    \n",
    "    # Re-tokenize to ensure the total length is within the limit\n",
    "    start_tokenized_ids = tokenizer(start_text, add_special_tokens=False).input_ids\n",
    "    end_tokenized_ids = tokenizer(end_text, add_special_tokens=False).input_ids\n",
    "\n",
    "    # Adjust the lengths to fit within the max_len\n",
    "    while len(start_tokenized_ids) + len(end_tokenized_ids) > max_len:\n",
    "        if len(start_tokenized_ids) > len(end_tokenized_ids):\n",
    "            start_tokenized_ids.pop()\n",
    "        else:\n",
    "            end_tokenized_ids.pop(0)\n",
    "    \n",
    "    # Combine the adjusted parts\n",
    "    adjusted_text = tokenizer.decode(start_tokenized_ids) + ' ... ' + tokenizer.decode(end_tokenized_ids)\n",
    "    \n",
    "    return adjusted_text\n",
    "\n",
    "    \n",
    "class MME_dataset(Dataset):\n",
    "    def __init__(self, data_prefix, anno_path, num_segments=16, resolution=224, hd_num=6, max_subtitle_len=4096):\n",
    "        self.data_prefix = data_prefix\n",
    "        with open(anno_path, 'r') as f:\n",
    "            self.data_list = json.load(f)\n",
    "        \n",
    "        self.hd_num = hd_num\n",
    "        self.num_segments = num_segments\n",
    "        self.resolution = resolution\n",
    "        self.max_subtitle_len = max_subtitle_len\n",
    "\n",
    "        # transform\n",
    "        mean = (0.485, 0.456, 0.406)\n",
    "        std = (0.229, 0.224, 0.225)\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Lambda(lambda x: x.float().div(255.0)),\n",
    "            transforms.Normalize(mean, std)\n",
    "        ])\n",
    "        self.hd_transform = HD_transform_no_padding\n",
    "    \n",
    "    def __str__(self):\n",
    "        task_dict = {}\n",
    "        total = 0\n",
    "        for data in self.data_list:\n",
    "            if data['duration_category'] not in ans_dict:\n",
    "                task_dict[data['duration_category']] = {}\n",
    "            for q in data['questions']:\n",
    "                if q['task_type'] not in ans_dict[data['duration_category']]:\n",
    "                    ans_dict[data['duration_category']][q['task_type']] = 0\n",
    "                ans_dict[data['duration_category']][q['task_type']] += 1\n",
    "                total += 1\n",
    "\n",
    "        res = f\"There are {len(self.data_list)} videos.\\n\"\n",
    "        res += f\"There are {total} QAs.\\n\"\n",
    "        for k, v in task_dict.items():\n",
    "            res += f\"------{k}------\\n\"\n",
    "            for kk, vv in task_dict.items():\n",
    "                res += f\"{kk}: {vv}\\n\"\n",
    "                \n",
    "        return res.rstrip()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def get_index(self, bound, fps, max_frame, first_idx=0):\n",
    "        if bound:\n",
    "            start, end = bound[0], bound[1]\n",
    "        else:\n",
    "            start, end = -100000, 100000\n",
    "        start_idx = max(first_idx, round(start * fps))\n",
    "        end_idx = min(round(end * fps), max_frame)\n",
    "        seg_size = float(end_idx - start_idx) / self.num_segments\n",
    "        frame_indices = np.array([\n",
    "            int(start_idx + (seg_size / 2) + np.round(seg_size * idx))\n",
    "            for idx in range(self.num_segments)\n",
    "        ])\n",
    "        return frame_indices\n",
    "\n",
    "    def read_frame(self, video_path, bound=None):\n",
    "        video_path = os.path.join(video_path, str(self.num_segments))\n",
    "        \n",
    "        if os.path.exists(video_path):\n",
    "            frame_list = [p for p in os.listdir(video_path)]\n",
    "        else:\n",
    "            raise Exception\n",
    "            \n",
    "        images_group = list()\n",
    "        \n",
    "        for frame_name in frame_list:\n",
    "            img = Image.open(os.path.join(video_path, frame_name))\n",
    "            img = PILToTensor()(img).unsqueeze(0)\n",
    "            img = self.hd_transform(img.float(), image_size=self.resolution, hd_num=self.hd_num)\n",
    "            images_group.append(img)\n",
    "        torch_imgs = self.transform(torch.vstack(images_group))\n",
    "        return torch_imgs\n",
    "    \n",
    "    def read_video(self, video_path, bound=None):\n",
    "        vr = VideoReader(video_path, ctx=cpu(0), num_threads=1)\n",
    "        max_frame = len(vr) - 1\n",
    "        fps = float(vr.get_avg_fps())\n",
    "        frame_indices = self.get_index(bound, fps, max_frame, first_idx=0) \n",
    "\n",
    "        frames = vr.get_batch(frame_indices)\n",
    "        frames = frames.permute(0, 3, 1, 2)\n",
    "        frames = self.hd_transform(frames.float(), image_size=self.resolution, hd_num=self.hd_num)\n",
    "        torch_imgs = self.transform(frames)\n",
    "        return torch_imgs\n",
    "\n",
    "    def qa_template(self, data):\n",
    "        question = f\"Question: {data['question']}\\n\"\n",
    "        question += \"Options:\\n\"\n",
    "        answer = data['answer']\n",
    "        answer = f\"({answer}) {data['choices'][ord(answer) - ord('A')][3:]}\"\n",
    "        for idx, c in enumerate(data['choices']):\n",
    "            cur_choice, cur_text = c[0], c[3:]\n",
    "            question += f\"({cur_choice}) {cur_text}\\n\"\n",
    "        question = question.rstrip()\n",
    "        return question, answer\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_name = self.data_list[idx]['url'].split(\"watch?v=\")[1]\n",
    "        video_path = os.path.join(self.data_prefix, \"frames\", video_name)\n",
    "\n",
    "        # We store the videos with only 16 or 32 frames for testing,\n",
    "        # since directly reading the whold videos cost a lot of time.\n",
    "        # You can also read the whole video via self.read_video(video_path)\n",
    "        torch_imgs = self.read_frame(video_path)\n",
    "        duration_category = self.data_list[idx]['duration_category']\n",
    "        qa_list = []\n",
    "        for qa in self.data_list[idx]['questions']:\n",
    "            qa_list.append(self.qa_template(qa))\n",
    "\n",
    "        subtitle = \"\"\n",
    "        try:\n",
    "            subtitle_path = os.path.join(self.data_prefix, \"subtitle\", video_name + \".vtt\")\n",
    "            if os.path.exists(subtitle_path):\n",
    "                subtitle = read_vtt_and_concatenate(subtitle_path, model.mistral_tokenizer, self.max_subtitle_len)\n",
    "        except Exception:\n",
    "            subtitle = \"\"\n",
    "            print(f\"Error for {subtitle_path}\")\n",
    "            \n",
    "        return {\n",
    "            'subtitle': subtitle,\n",
    "            'video': torch_imgs, \n",
    "            'qa_list': qa_list,\n",
    "            'duration_category': duration_category\n",
    "        }\n",
    "    \n",
    "\n",
    "def infer_mme(\n",
    "        data_sample, system=\"\", \n",
    "        question_prompt='', # add in the end of question\n",
    "        answer_prompt=None, # add in the begining of answer\n",
    "        return_prompt='',  # add in the begining of return message\n",
    "        system_q=False, # whether add question in the system prompt for QFormer\n",
    "        print_res=True,\n",
    "        system_llm=False,\n",
    "        add_subtitle=False,\n",
    "    ):\n",
    "    assert system_q == False, \"do not support system_q now\"\n",
    "    video = data_sample[\"video\"]\n",
    "    T_, C, H, W = video.shape\n",
    "    video = video.reshape(1, T_, C, H, W).to(\"cuda:0\")\n",
    "    \n",
    "    video_list = []\n",
    "    with torch.no_grad():\n",
    "        if system_q:\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            video_emb, _, _ = model.encode_img(video, system)\n",
    "    video_list.append(video_emb[0])\n",
    "\n",
    "    pred_list = []\n",
    "    gt_list = []\n",
    "    for idx, qa in enumerate(data_sample['qa_list']):\n",
    "        print(f\"----------qa_{idx}---------\", flush=True)\n",
    "        chat = EasyDict({\n",
    "            \"system\": system,\n",
    "            \"roles\": (\"[INST]\", \"[/INST]\"),\n",
    "            \"messages\": [],\n",
    "            \"sep\": \"\"\n",
    "        })\n",
    "    \n",
    "        if add_subtitle:\n",
    "            if data_sample['subtitle'] != '':\n",
    "                subtitle = f\"This video's subtitles are listed below: {data_sample['subtitle']}\"\n",
    "                chat.messages.append([chat.roles[0], f\"{subtitle}\\n<Video><VideoHere></Video> [/INST]\"])\n",
    "            else:\n",
    "                chat.messages.append([chat.roles[0], f\"<Video><VideoHere></Video> [/INST]\"])\n",
    "        else:\n",
    "            chat.messages.append([chat.roles[0], f\"<Video><VideoHere></Video> [/INST]\"])\n",
    "    \n",
    "        if system_llm:\n",
    "            prompt = system + qa[0] + question_prompt\n",
    "        else:\n",
    "            prompt = qa[0] + question_prompt\n",
    "        \n",
    "        ask(prompt, chat)\n",
    "    \n",
    "        llm_message = answer(\n",
    "            conv=chat, model=model, do_sample=False, \n",
    "            img_list=video_list, max_new_tokens=100, \n",
    "            answer_prompt=answer_prompt, print_res=print_res\n",
    "        )[0]\n",
    "        # remove potential explanation\n",
    "        llm_message = return_prompt + llm_message.strip().split('\\n')[0]\n",
    "        print(f\"Pred: {llm_message}\", flush=True)\n",
    "        print(f\"GT: {qa[1]}\", flush=True)\n",
    "        pred_list.append(llm_message[1])\n",
    "        gt_list.append(qa[1][1])\n",
    "    return pred_list, gt_list\n",
    "\n",
    "    \n",
    "#  position embedding\n",
    "num_frame = 16\n",
    "resolution = 224\n",
    "new_pos_emb = get_sinusoid_encoding_table(n_position=(resolution//16)**2*num_frame, cur_frame=num_frame)\n",
    "model.vision_encoder.encoder.pos_embed = new_pos_emb\n",
    "\n",
    "data_dir = \"your_data_path/videomme\"\n",
    "anno_path =  \"your_data_path/Video-MME.json\"\n",
    "dataset = MME_dataset(\n",
    "    data_dir, \n",
    "    anno_path, \n",
    "    num_segments=num_frame, resolution=resolution\n",
    ")\n",
    "\n",
    "with open(anno_path, 'r') as f:\n",
    "    res_json_data = json.load(f)\n",
    "\n",
    "save_path = \"./demo/videomme/your_prediction\"\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "res_list = []\n",
    "acc_dict = {}\n",
    "\n",
    "for idx, example in enumerate(tqdm(dataset)):\n",
    "    duration_category = example['duration_category']\n",
    "    if duration_category not in acc_dict:\n",
    "        acc_dict[duration_category] = [0, 0] # correct, total\n",
    "    qa_count = len(example['qa_list'])\n",
    "    acc_dict[duration_category][1] += qa_count\n",
    "    total += qa_count\n",
    "    pred_list, gt_list = infer_mme(\n",
    "        example, \n",
    "        \"Carefully watch the video and pay attention to the cause and sequence of events, the detail and movement of objects, and the action and pose of persons. Based on your observations, select the best option that accurately addresses the question.\\n\",\n",
    "        question_prompt=\"\\nOnly give the best option.\",\n",
    "        answer_prompt=\"Best option:(\",\n",
    "        return_prompt='(',\n",
    "        system_q=False,\n",
    "        print_res=False,\n",
    "        system_llm=True,\n",
    "        # add_subtitle=True, # Comment this line to add subtitles, we use the whole subtitles by default.\n",
    "    )\n",
    "    res_list.append({\n",
    "        'pred': pred_list,\n",
    "        'gt': gt_list\n",
    "    })\n",
    "    qa_idx = 0\n",
    "    for pred, gt in zip(pred_list, gt_list):\n",
    "        if pred == gt:\n",
    "            acc_dict[duration_category][0] += 1\n",
    "            correct += 1\n",
    "        res_json_data[idx]['questions'][qa_idx]['response'] = pred\n",
    "        qa_idx += 1\n",
    "    print(f\"Part  Acc: {acc_dict[duration_category][0] / acc_dict[duration_category][1] * 100 :.2f}%\")\n",
    "    print(f\"Total Acc: {correct / total * 100 :.2f}%\")\n",
    "    print('-' * 50, duration_category, '-' * 50)\n",
    "\n",
    "with open(f\"{save_path}.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"acc_dict\": acc_dict,\n",
    "        \"res_list\": res_list\n",
    "    }, f)\n",
    "\n",
    "with open(f\"{save_path}_full.json\", \"w\") as f:\n",
    "    json.dump(res_json_data, f)\n",
    "\n",
    "# Then you can run https://github.com/BradyFU/Video-MME/blob/main/evaluation/eval_your_results.py to get the score\n",
    "# python3 eval.py --results_file your_prediction_full.json --video_duration_type short,medium,long"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
